{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0b5ab195",
      "metadata": {
        "id": "0b5ab195"
      },
      "outputs": [],
      "source": [
        "# @title Run profile\n",
        "\n",
        "RUN_MODE = \"quick\"   # \"quick\" or \"budget_100\"\n",
        "\n",
        "# You can always override individual values later.\n",
        "\n",
        "if RUN_MODE == \"quick\":\n",
        "    # Small + fast: good for verifying everything end-to-end\n",
        "    TRAIN_EXAMPLES = 500\n",
        "    VAL_EXAMPLES   = 20\n",
        "    TOKENIZER_TRAIN_EXAMPLES = 300\n",
        "\n",
        "    SEQ_LEN = 256\n",
        "    VOCAB_SIZE = 8_000\n",
        "\n",
        "    D_MODEL = 384\n",
        "    N_LAYERS = 6\n",
        "    N_HEADS = 6\n",
        "    D_FF = 4 * D_MODEL\n",
        "\n",
        "    DIFFUSION_STEPS = 32\n",
        "\n",
        "    TRAIN_STEPS = 200\n",
        "    BATCH_SIZE = 32\n",
        "    GRAD_ACCUM = 1\n",
        "    LR = 3e-4\n",
        "    WEIGHT_DECAY = 0.1\n",
        "    WARMUP_STEPS = 200\n",
        "\n",
        "elif RUN_MODE == \"budget_100\":\n",
        "    # Heavier: better quality, uses more compute\n",
        "    TRAIN_EXAMPLES = 1000_000\n",
        "    VAL_EXAMPLES   = 10_000\n",
        "    TOKENIZER_TRAIN_EXAMPLES = 150_000\n",
        "\n",
        "    SEQ_LEN = 256\n",
        "    VOCAB_SIZE = 26_000\n",
        "\n",
        "    D_MODEL = 512\n",
        "    N_LAYERS = 10\n",
        "    N_HEADS = 8\n",
        "    D_FF = 4 * D_MODEL\n",
        "\n",
        "    DIFFUSION_STEPS = 128\n",
        "\n",
        "    TRAIN_STEPS = 50000\n",
        "    BATCH_SIZE = 32\n",
        "    GRAD_ACCUM = 2\n",
        "    LR = 2e-4\n",
        "    WEIGHT_DECAY = 0.1\n",
        "    WARMUP_STEPS = 1_000\n",
        "\n",
        "else:\n",
        "    raise ValueError(\"RUN_MODE must be 'quick' or 'budget_100'\")\n",
        "\n",
        "print(\"RUN_MODE:\", RUN_MODE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "98504dbb",
      "metadata": {
        "id": "98504dbb"
      },
      "outputs": [],
      "source": [
        "# @title Install dependencies\n",
        "!pip -q install -U datasets tokenizers accelerate tqdm numpy einops imageio pillow transformers\n",
        "!pip install hf_transfer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fb098d2c",
      "metadata": {
        "id": "fb098d2c"
      },
      "outputs": [],
      "source": [
        "# @title Import dependencies\n",
        "import os, math, time, json, random\n",
        "import numpy as np\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import IterableDataset, DataLoader\n",
        "\n",
        "from datasets import load_dataset\n",
        "\n",
        "print(\"torch:\", torch.__version__)\n",
        "print(\"cuda available:\", torch.cuda.is_available())\n",
        "if torch.cuda.is_available():\n",
        "    print(\"GPU:\", torch.cuda.get_device_name(0))\n",
        "    print(\"bf16 supported:\", torch.cuda.is_bf16_supported())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b5486dbb",
      "metadata": {
        "id": "b5486dbb"
      },
      "outputs": [],
      "source": [
        "# @title Load dataset\n",
        "!pip uninstall numpy -y --quiet\n",
        "!pip install numpy==1.23.5 --quiet\n",
        "train_ds = load_dataset(\"roneneldan/TinyStories\", split=f\"train[:{TRAIN_EXAMPLES}]\")\n",
        "val_ds   = load_dataset(\"roneneldan/TinyStories\", split=f\"validation[:{VAL_EXAMPLES}]\")\n",
        "\n",
        "print(train_ds, val_ds)\n",
        "print(\"\\nExample:\\n\", train_ds[0][\"text\"][:500])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7d378543",
      "metadata": {
        "id": "7d378543"
      },
      "outputs": [],
      "source": [
        "# @title Train tokenizer\n",
        "from tokenizers import Tokenizer\n",
        "from tokenizers.models import BPE\n",
        "from tokenizers.trainers import BpeTrainer\n",
        "from tokenizers.pre_tokenizers import ByteLevel\n",
        "from tokenizers.decoders import ByteLevel as ByteLevelDecoder\n",
        "from tokenizers.normalizers import NFKC\n",
        "from tokenizers.processors import TemplateProcessing\n",
        "\n",
        "SPECIAL_TOKENS = [\n",
        "    \"[PAD]\", \"[UNK]\", \"[BOS]\", \"[EOS]\", \"[MASK]\",\n",
        "    \"<|user|>\", \"<|assistant|>\", \"<|system|>\", \"<|end|>\",\n",
        "]\n",
        "\n",
        "def tokenizer_training_iterator(ds, n_examples):\n",
        "    for i in range(min(n_examples, len(ds))):\n",
        "        story = ds[i][\"text\"].strip()\n",
        "        yield f\"<|user|>\\nWrite a short story.\\n<|assistant|>\\n{story}\\n<|end|>\\n\"\n",
        "\n",
        "tokenizer = Tokenizer(BPE(unk_token=\"[UNK]\"))\n",
        "tokenizer.normalizer = NFKC()\n",
        "tokenizer.pre_tokenizer = ByteLevel(add_prefix_space=False)\n",
        "\n",
        "trainer = BpeTrainer(\n",
        "    vocab_size=VOCAB_SIZE,\n",
        "    min_frequency=2,\n",
        "    special_tokens=SPECIAL_TOKENS,\n",
        ")\n",
        "\n",
        "print(\"Training tokenizer...\")\n",
        "tokenizer.train_from_iterator(\n",
        "    tokenizer_training_iterator(train_ds, TOKENIZER_TRAIN_EXAMPLES),\n",
        "    trainer=trainer\n",
        ")\n",
        "\n",
        "bos_id = tokenizer.token_to_id(\"[BOS]\")\n",
        "eos_id = tokenizer.token_to_id(\"[EOS]\")\n",
        "tokenizer.post_processor = TemplateProcessing(\n",
        "    single=\"[BOS] $A [EOS]\",\n",
        "    special_tokens=[(\"[BOS]\", bos_id), (\"[EOS]\", eos_id)],\n",
        ")\n",
        "tokenizer.decoder = ByteLevelDecoder()\n",
        "\n",
        "TOKENIZER_DIR = \"tokenizer_from_scratch\"\n",
        "os.makedirs(TOKENIZER_DIR, exist_ok=True)\n",
        "TOKENIZER_FILE = os.path.join(TOKENIZER_DIR, \"tokenizer.json\")\n",
        "tokenizer.save(TOKENIZER_FILE)\n",
        "\n",
        "print(\"Saved tokenizer to:\", TOKENIZER_FILE)\n",
        "print(\"Vocab size:\", tokenizer.get_vocab_size())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a8e3ac1e",
      "metadata": {
        "id": "a8e3ac1e"
      },
      "outputs": [],
      "source": [
        "#@title Load tokenizer\n",
        "!pip install transformers -U --quiet\n",
        "from transformers import PreTrainedTokenizerFast\n",
        "\n",
        "hf_tokenizer = PreTrainedTokenizerFast(tokenizer_file=TOKENIZER_FILE)\n",
        "\n",
        "hf_tokenizer.pad_token  = \"[PAD]\"\n",
        "hf_tokenizer.unk_token  = \"[UNK]\"\n",
        "hf_tokenizer.bos_token  = \"[BOS]\"\n",
        "hf_tokenizer.eos_token  = \"[EOS]\"\n",
        "hf_tokenizer.mask_token = \"[MASK]\"\n",
        "\n",
        "hf_tokenizer.add_special_tokens({\n",
        "    \"additional_special_tokens\": [\"<|user|>\", \"<|assistant|>\", \"<|system|>\", \"<|end|>\"]\n",
        "})\n",
        "\n",
        "PAD_ID  = hf_tokenizer.pad_token_id\n",
        "MASK_ID = hf_tokenizer.mask_token_id\n",
        "BOS_ID  = hf_tokenizer.bos_token_id\n",
        "EOS_ID  = hf_tokenizer.eos_token_id\n",
        "\n",
        "print(\"PAD_ID:\", PAD_ID, \"MASK_ID:\", MASK_ID, \"BOS_ID:\", BOS_ID, \"EOS_ID:\", EOS_ID)\n",
        "print(\"Example encoding:\", hf_tokenizer.encode(\"Hello world!\")[:20])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2ee399aa",
      "metadata": {},
      "outputs": [],
      "source": [
        "#@title DiffusionLMConfig - THe main Diffusion Model from Transformar Model\n",
        "from dataclasses import dataclass\n",
        "\n",
        "@dataclass\n",
        "class DiffusionLMConfig:\n",
        "    vocab_size: int\n",
        "    seq_len: int # Context Block\n",
        "    d_model: int # Head Dimension\n",
        "    n_layers: int\n",
        "    n_heads: int\n",
        "    d_ff: int #Feed Forward \n",
        "    dropout: float\n",
        "    diffusion_steps: int # Diffusion Steps\n",
        "\n",
        "class DiffusionTransformerLM(nn.Module):\n",
        "    def __init__(self, cfg: DiffusionLMConfig):\n",
        "        super().__init__()\n",
        "        self.cfg = cfg\n",
        "\n",
        "        self.tok_emb = nn.Embedding(cfg.vocab_size, cfg.d_model)\n",
        "        self.pos_emb = nn.Embedding(cfg.seq_len, cfg.d_model)\n",
        "        self.time_emb = nn.Embedding(cfg.diffusion_steps + 1, cfg.d_model)\n",
        "\n",
        "        enc_layer = nn.TransformerEncoderLayer(\n",
        "            d_model=cfg.d_model,\n",
        "            nhead=cfg.n_heads,\n",
        "            dim_feedforward=cfg.d_ff,\n",
        "            dropout=cfg.dropout,\n",
        "            batch_first=True,\n",
        "            activation=\"gelu\",\n",
        "            norm_first=True,\n",
        "        )\n",
        "        self.encoder = nn.TransformerEncoder(enc_layer, num_layers=cfg.n_layers)\n",
        "        self.ln_f = nn.LayerNorm(cfg.d_model)\n",
        "        self.lm_head = nn.Linear(cfg.d_model, cfg.vocab_size, bias=False)\n",
        "\n",
        "        # Tie weights (optional; common in LMs) # Weights Sharing\n",
        "        self.lm_head.weight = self.tok_emb.weight\n",
        "\n",
        "        self.drop = nn.Dropout(cfg.dropout)\n",
        "\n",
        "    def forward(self, input_ids, timesteps, attention_mask=None):\n",
        "        # input_ids: [B, L]\n",
        "        # timesteps: [B] integer diffusion step in [1..T]\n",
        "        # attention_mask: [B, L] bool, True for non-pad tokens\n",
        "\n",
        "        B, L = input_ids.shape\n",
        "        if L > self.cfg.seq_len:\n",
        "            raise ValueError(f\"Sequence length {L} > cfg.seq_len {self.cfg.seq_len}\")\n",
        "\n",
        "        pos = torch.arange(L, device=input_ids.device).unsqueeze(0)  # [1, L]\n",
        "        x = self.tok_emb(input_ids) + self.pos_emb(pos)\n",
        "\n",
        "        t_emb = self.time_emb(timesteps).unsqueeze(1)  # [B, 1, D]\n",
        "        x = x + t_emb\n",
        "        x = self.drop(x)\n",
        "\n",
        "        if attention_mask is None:\n",
        "            src_key_padding_mask = None\n",
        "        else:\n",
        "            src_key_padding_mask = ~attention_mask  # invert: True = pad/ignore\n",
        "\n",
        "        x = self.encoder(x, src_key_padding_mask=src_key_padding_mask)\n",
        "        x = self.ln_f(x)\n",
        "        logits = self.lm_head(x)  # [B, L, V]\n",
        "        return logits\n",
        "\n",
        "cfg = DiffusionLMConfig(\n",
        "    vocab_size=len(hf_tokenizer),\n",
        "    seq_len=SEQ_LEN,\n",
        "    d_model=D_MODEL,\n",
        "    n_layers=N_LAYERS,\n",
        "    n_heads=N_HEADS,\n",
        "    d_ff=D_FF,\n",
        "    dropout=0.1,\n",
        "    diffusion_steps=DIFFUSION_STEPS,\n",
        ")\n",
        "model = DiffusionTransformerLM(cfg)\n",
        "\n",
        "n_params = sum(p.numel() for p in model.parameters())\n",
        "print(f\"Model parameters: {n_params/1e6:.2f}M\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f1ce95b4",
      "metadata": {},
      "outputs": [],
      "source": [
        "#@title Format as chat\n",
        "def format_as_chat(story_text: str) -> str:\n",
        "    story_text = story_text.strip()\n",
        "    return f\"<|user|>\\nWrite a short story.\\n<|assistant|>\\n{story_text}\\n<|end|>\\n\"\n",
        "\n",
        "class TokenBlockDataset(IterableDataset):\n",
        "    def __init__(self, hf_ds, tokenizer, seq_len, shuffle=False, seed=0):\n",
        "        self.hf_ds = hf_ds\n",
        "        self.tokenizer = tokenizer\n",
        "        self.seq_len = seq_len\n",
        "        self.shuffle = shuffle\n",
        "        self.seed = seed\n",
        "\n",
        "    def __iter__(self):\n",
        "        indices = list(range(len(self.hf_ds)))\n",
        "        if self.shuffle:\n",
        "            rng = random.Random(self.seed)\n",
        "            rng.shuffle(indices)\n",
        "\n",
        "        buffer = []\n",
        "        for idx in indices:\n",
        "            text = format_as_chat(self.hf_ds[idx][\"text\"])\n",
        "            ids = self.tokenizer.encode(text, add_special_tokens=True)\n",
        "            buffer.extend(ids)\n",
        "\n",
        "            while len(buffer) >= self.seq_len:\n",
        "                block = buffer[:self.seq_len]\n",
        "                buffer = buffer[self.seq_len:]\n",
        "                yield torch.tensor(block, dtype=torch.long)\n",
        "\n",
        "train_blocks = TokenBlockDataset(train_ds, hf_tokenizer, SEQ_LEN, shuffle=True, seed=42)\n",
        "val_blocks   = TokenBlockDataset(val_ds,   hf_tokenizer, SEQ_LEN, shuffle=False)\n",
        "\n",
        "def collate_blocks(batch):\n",
        "    input_ids = torch.stack(batch, dim=0)  # [B, L]\n",
        "    attention_mask = (input_ids != PAD_ID)\n",
        "    return {\"input_ids\": input_ids, \"attention_mask\": attention_mask}\n",
        "\n",
        "train_loader = DataLoader(train_blocks, batch_size=BATCH_SIZE, collate_fn=collate_blocks)\n",
        "val_loader   = DataLoader(val_blocks,   batch_size=BATCH_SIZE, collate_fn=collate_blocks)\n",
        "\n",
        "b = next(iter(train_loader))\n",
        "print({k: v.shape for k, v in b.items()})\n",
        "print(\"Decoded snippet:\\n\", hf_tokenizer.decode(b[\"input_ids\"][0][:120].tolist()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "772fafc5",
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import math\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Technical Standard: Offset 's' prevents immediate information collapse at t=0\n",
        "# Value 0.008 is the established industry standard for 1024-step diffusion.\n",
        "S_OFFSET = 0.008\n",
        "\n",
        "def cosine_mask_ratio_schedule(t: torch.Tensor, T: int):\n",
        "    \"\"\"\n",
        "    Implements the squared cosine schedule for discrete token masking.\n",
        "    \n",
        "    The function calculates the ratio of tokens to be masked based on the \n",
        "    Nichol & Dhariwal (2021) formulation, normalized for a [0, 1] range.\n",
        "    \"\"\"\n",
        "    # Normalize timesteps to [0, 1]\n",
        "    t_over_T = t.float() / float(T)\n",
        "    \n",
        "    # Calculate f(t) using the squared cosine formula\n",
        "    # We apply an offset to ensure unmasked_ratio(0) is approximately 1\n",
        "    def f(tau):\n",
        "        return torch.cos(((tau + S_OFFSET) / (1 + S_OFFSET)) * (math.pi / 2)) ** 2\n",
        "\n",
        "    unmasked_ratio_t = f(t_over_T)\n",
        "    unmasked_ratio_0 = f(torch.zeros_like(t_over_T))\n",
        "    \n",
        "    # The mask ratio is 1 minus the percentage of tokens remaining unmasked\n",
        "    # We clamp to [0, 1] to prevent floating point precision errors\n",
        "    mask_ratio = 1 - (unmasked_ratio_t / unmasked_ratio_0)\n",
        "    return torch.clamp(mask_ratio, 0.0, 1.0)\n",
        "\n",
        "@torch.no_grad()\n",
        "def corrupt_with_mask_cosine(input_ids, attention_mask, t, mask_token_id: int, T: int):\n",
        "    \"\"\"\n",
        "    Surgically corrupts the input manifold using a Cosine Schedule.\n",
        "    \n",
        "    Technical Standards:\n",
        "    1. Special Token Protection: Explicitly prevents [BOS], [EOS], and [PAD] corruption.\n",
        "    2. Numerical Stability: Uses a high-precision mask ratio calculation.\n",
        "    \"\"\"\n",
        "    B, L = input_ids.shape\n",
        "    \n",
        "    # Calculate the non-linear mask ratio for the current batch of timesteps\n",
        "    # ratio shape: [B, 1] for broadcasting across the sequence length\n",
        "    ratio = cosine_mask_ratio_schedule(t, T).unsqueeze(1)\n",
        "\n",
        "    # Forensic Guard: Protect the structural anchors of the sequence\n",
        "    # These tokens must never be masked to preserve bidirectional RoPE stability\n",
        "    can_mask = attention_mask.clone().bool()\n",
        "    can_mask &= (input_ids != BOS_ID) & (input_ids != EOS_ID) & (input_ids != PAD_ID)\n",
        "\n",
        "    # Generate the Bernoulli manifold for masking\n",
        "    rand = torch.rand((B, L), device=input_ids.device)\n",
        "    mask_positions = (rand < ratio) & can_mask\n",
        "\n",
        "    # Apply the [MASK] tokens to the isolated manifold\n",
        "    noisy = input_ids.clone()\n",
        "    noisy[mask_positions] = mask_token_id\n",
        "\n",
        "    # Construct the training labels\n",
        "    # Standard: Use -100 for unmasked positions to be ignored by Cross-Entropy\n",
        "    labels = torch.full_like(input_ids, -100)\n",
        "    labels[mask_positions] = input_ids[mask_positions]\n",
        "\n",
        "    return noisy, labels, mask_positions\n",
        "\n",
        "def diffusion_loss_cosine(model, batch, T: int):\n",
        "    \"\"\"\n",
        "    Calculates the weighted cross-entropy loss under a Cosine noise schedule.\n",
        "    \n",
        "    Note: For Dream 7B, the model utilizes bidirectional attention, so we pass\n",
        "    the full attention_mask to the forward method.\n",
        "    \"\"\"\n",
        "    input_ids = batch[\"input_ids\"]\n",
        "    attention_mask = batch[\"attention_mask\"]\n",
        "\n",
        "    B = input_ids.size(0)\n",
        "    # Uniformly sample timesteps to optimize the entire denoising trajectory\n",
        "    t = torch.randint(1, T + 1, (B,), device=input_ids.device)\n",
        "\n",
        "    noisy_ids, labels, _ = corrupt_with_mask_cosine(\n",
        "        input_ids=input_ids,\n",
        "        attention_mask=attention_mask,\n",
        "        t=t,\n",
        "        mask_token_id=MASK_ID,\n",
        "        T=T,\n",
        "    )\n",
        "\n",
        "    # Forward pass through the bidirectional transformer backbone\n",
        "    # timesteps are used by the model for context-adaptive conditioning\n",
        "    logits = model(noisy_ids, timesteps=t, attention_mask=attention_mask)\n",
        "    \n",
        "    # Surgical Loss Calculation: Only penalize incorrect denoising of [MASK] positions\n",
        "    loss = F.cross_entropy(\n",
        "        logits.view(-1, logits.size(-1)),\n",
        "        labels.view(-1),\n",
        "        ignore_index=-100,\n",
        "    )\n",
        "    return loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1718155b",
      "metadata": {},
      "outputs": [],
      "source": [
        "#@title Prepare for training\n",
        "from accelerate import Accelerator\n",
        "from transformers import get_cosine_schedule_with_warmup\n",
        "\n",
        "accelerator = Accelerator(mixed_precision=\"bf16\" if torch.cuda.is_available() and torch.cuda.is_bf16_supported() else \"fp16\")\n",
        "device = accelerator.device\n",
        "\n",
        "model = model.to(device)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n",
        "\n",
        "scheduler = get_cosine_schedule_with_warmup(\n",
        "    optimizer,\n",
        "    num_warmup_steps=WARMUP_STEPS,\n",
        "    num_training_steps=TRAIN_STEPS,\n",
        ")\n",
        "\n",
        "model, optimizer, train_loader, val_loader, scheduler = accelerator.prepare(\n",
        "    model, optimizer, train_loader, val_loader, scheduler\n",
        ")\n",
        "\n",
        "def eval_loss(n_batches=20):\n",
        "    model.eval()\n",
        "    losses = []\n",
        "    with torch.no_grad():\n",
        "        for i, batch in enumerate(val_loader):\n",
        "            if i >= n_batches:\n",
        "                break\n",
        "\n",
        "            loss = diffusion_loss(model, batch, T=cfg.diffusion_steps)\n",
        "\n",
        "            # gather across processes -> always make it 1D\n",
        "            gathered = accelerator.gather(loss.detach().float().reshape(1))\n",
        "\n",
        "            # now gathered is shape [world_size] (or [1] on single GPU)\n",
        "            losses.append(gathered.cpu())\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    if len(losses) == 0:\n",
        "        return float(\"nan\")\n",
        "\n",
        "    losses = torch.cat(losses)   # safe: all are 1D tensors\n",
        "    return losses.mean().item()\n",
        "\n",
        "\n",
        "model.train()\n",
        "pbar = tqdm(range(TRAIN_STEPS), disable=not accelerator.is_main_process)\n",
        "running = []\n",
        "\n",
        "train_iter = iter(train_loader)\n",
        "\n",
        "for step in pbar:\n",
        "    try:\n",
        "        batch = next(train_iter)\n",
        "    except StopIteration:\n",
        "        train_iter = iter(train_loader)\n",
        "        batch = next(train_iter)\n",
        "\n",
        "    loss = diffusion_loss(model, batch, T=cfg.diffusion_steps) / GRAD_ACCUM\n",
        "    accelerator.backward(loss)\n",
        "\n",
        "    if (step + 1) % GRAD_ACCUM == 0:\n",
        "        accelerator.clip_grad_norm_(model.parameters(), 1.0)\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "    running.append(loss.item() * GRAD_ACCUM)\n",
        "\n",
        "    if (step + 1) % 50 == 0 and accelerator.is_main_process:\n",
        "        pbar.set_description(f\"loss={np.mean(running[-50:]):.4f} lr={scheduler.get_last_lr()[0]:.2e}\")\n",
        "\n",
        "    if (step + 1) % 500 == 0 and accelerator.is_main_process:\n",
        "        val_l = eval_loss(n_batches=10)\n",
        "        print(f\"\\nStep {step+1} | val_loss ~ {val_l:.4f}\")\n",
        "\n",
        "if accelerator.is_main_process:\n",
        "    OUT_DIR = \"checkpoints/final\"\n",
        "    os.makedirs(OUT_DIR, exist_ok=True)\n",
        "    torch.save(accelerator.unwrap_model(model).state_dict(), os.path.join(OUT_DIR, \"model.pt\"))\n",
        "    with open(os.path.join(OUT_DIR, \"config.json\"), \"w\") as f:\n",
        "        json.dump(cfg.__dict__, f, indent=2)\n",
        "    hf_tokenizer.save_pretrained(os.path.join(OUT_DIR, \"tokenizer\"))\n",
        "    print(\"Saved final checkpoint to:\", OUT_DIR)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
