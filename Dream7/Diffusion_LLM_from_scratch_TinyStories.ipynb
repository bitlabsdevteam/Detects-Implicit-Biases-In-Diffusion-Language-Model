{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "c70595ce",
      "metadata": {
        "id": "c70595ce"
      },
      "source": [
        "# Diffusion Language Model from scratch (TinyStories) + terminal-style inference GIF\n",
        "\n",
        "This Colab notebook trains a **discrete diffusion language model** (a masked-denoising Transformer) **from random initialization** on a **small slice of TinyStories**, then runs **diffusion sampling** and exports a **terminal-looking inference GIF** similar to the one you shared.\n",
        "\n",
        "It’s designed to be:\n",
        "\n",
        "- **Educational**: you’ll see exactly what “diffusion for text” is doing at every step.\n",
        "- **Practical**: runs on a single GPU and saves only the final checkpoint by default.\n",
        "- **Hackable**: a few variables at the top control dataset size, model size, and training budget.\n",
        "\n",
        "---\n",
        "\n",
        "## What we’re building (high level)\n",
        "\n",
        "Autoregressive (AR) LMs generate **left-to-right**:\n",
        "\n",
        "> token₁ → token₂ → token₃ → …\n",
        "\n",
        "A masked diffusion LM generates by **iteratively denoising a whole sequence in parallel**:\n",
        "\n",
        "1. Start from a sequence that is mostly (or entirely) `[MASK]`.\n",
        "2. Predict tokens for all masked positions.\n",
        "3. Keep the most confident predictions, re-mask the uncertain ones.\n",
        "4. Repeat for many steps until no masks remain.\n",
        "\n",
        "This gives the “cool” effect where the output looks like it’s being *edited into existence*.\n",
        "\n",
        "---\n",
        "\n",
        "## Before you run\n",
        "\n",
        "1. **Runtime → Change runtime type → GPU**\n",
        "2. (Optional) Set **`RUN_MODE`** below:\n",
        "   - `\"quick\"`: small dataset slice, fewer steps (fast to see it work)\n",
        "   - `\"budget_100\"`: larger model/dataset/steps (better quality; costs more compute)\n",
        "\n",
        "> Note: TinyStories teaches the model to write simple stories.  \n",
        "> If you ask it for Python code, it will still try—but it may answer with story-like text until you train on code data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "0b5ab195",
      "metadata": {
        "id": "0b5ab195"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "RUN_MODE: quick\n"
          ]
        }
      ],
      "source": [
        "# =======================\n",
        "# 0) Choose a run profile\n",
        "# =======================\n",
        "\n",
        "RUN_MODE = \"quick\"   # \"quick\" or \"budget_100\"\n",
        "\n",
        "# You can always override individual values later.\n",
        "\n",
        "if RUN_MODE == \"quick\":\n",
        "    # Small + fast: good for verifying everything end-to-end\n",
        "    TRAIN_EXAMPLES = 500\n",
        "    VAL_EXAMPLES   = 20\n",
        "    TOKENIZER_TRAIN_EXAMPLES = 300\n",
        "\n",
        "    SEQ_LEN = 256\n",
        "    VOCAB_SIZE = 8_000\n",
        "\n",
        "    D_MODEL = 384\n",
        "    N_LAYERS = 6\n",
        "    N_HEADS = 6\n",
        "    D_FF = 4 * D_MODEL\n",
        "\n",
        "    DIFFUSION_STEPS = 32\n",
        "\n",
        "    TRAIN_STEPS = 200\n",
        "    BATCH_SIZE = 32\n",
        "    GRAD_ACCUM = 1\n",
        "    LR = 3e-4\n",
        "    WEIGHT_DECAY = 0.1\n",
        "    WARMUP_STEPS = 200\n",
        "\n",
        "elif RUN_MODE == \"budget_100\":\n",
        "    # Heavier: better quality, uses more compute\n",
        "    TRAIN_EXAMPLES = 1000_000\n",
        "    VAL_EXAMPLES   = 10_000\n",
        "    TOKENIZER_TRAIN_EXAMPLES = 150_000\n",
        "\n",
        "    SEQ_LEN = 256\n",
        "    VOCAB_SIZE = 26_000\n",
        "\n",
        "    D_MODEL = 512\n",
        "    N_LAYERS = 10\n",
        "    N_HEADS = 8\n",
        "    D_FF = 4 * D_MODEL\n",
        "\n",
        "    DIFFUSION_STEPS = 128\n",
        "\n",
        "    TRAIN_STEPS = 50000\n",
        "    BATCH_SIZE = 32\n",
        "    GRAD_ACCUM = 2\n",
        "    LR = 2e-4\n",
        "    WEIGHT_DECAY = 0.1\n",
        "    WARMUP_STEPS = 1_000\n",
        "\n",
        "else:\n",
        "    raise ValueError(\"RUN_MODE must be 'quick' or 'budget_100'\")\n",
        "\n",
        "print(\"RUN_MODE:\", RUN_MODE)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1a79586e",
      "metadata": {
        "id": "1a79586e"
      },
      "source": [
        "# 1) Install dependencies\n",
        "\n",
        "We’ll use:\n",
        "\n",
        "- `torch` for training\n",
        "- `datasets` for TinyStories\n",
        "- `tokenizers` to train a tokenizer **from scratch**\n",
        "- `tqdm`, `numpy`, `imageio`, `Pillow` for progress + video export"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "98504dbb",
      "metadata": {
        "id": "98504dbb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: hf_transfer in /Users/davidbong/Documents/AI_labs/DiffusionModel/Dream7B_LlaDA/.venv/lib/python3.12/site-packages (0.1.9)\n"
          ]
        }
      ],
      "source": [
        "!pip -q install -U datasets tokenizers accelerate tqdm numpy einops imageio pillow transformers\n",
        "!pip install hf_transfer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "fb098d2c",
      "metadata": {
        "id": "fb098d2c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch: 2.10.0\n",
            "cuda available: False\n"
          ]
        }
      ],
      "source": [
        "import os, math, time, json, random\n",
        "import numpy as np\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import IterableDataset, DataLoader\n",
        "\n",
        "from datasets import load_dataset\n",
        "\n",
        "print(\"torch:\", torch.__version__)\n",
        "print(\"cuda available:\", torch.cuda.is_available())\n",
        "if torch.cuda.is_available():\n",
        "    print(\"GPU:\", torch.cuda.get_device_name(0))\n",
        "    print(\"bf16 supported:\", torch.cuda.is_bf16_supported())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "b5486dbb",
      "metadata": {
        "id": "b5486dbb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mGetting requirements to build wheel\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m \u001b[31m[33 lines of output]\u001b[0m\n",
            "  \u001b[31m   \u001b[0m Traceback (most recent call last):\n",
            "  \u001b[31m   \u001b[0m   File \"/Users/davidbong/Documents/AI_labs/DiffusionModel/Dream7B_LlaDA/.venv/lib/python3.12/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py\", line 389, in <module>\n",
            "  \u001b[31m   \u001b[0m     main()\n",
            "  \u001b[31m   \u001b[0m   File \"/Users/davidbong/Documents/AI_labs/DiffusionModel/Dream7B_LlaDA/.venv/lib/python3.12/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py\", line 373, in main\n",
            "  \u001b[31m   \u001b[0m     json_out[\"return_val\"] = hook(**hook_input[\"kwargs\"])\n",
            "  \u001b[31m   \u001b[0m                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  \u001b[31m   \u001b[0m   File \"/Users/davidbong/Documents/AI_labs/DiffusionModel/Dream7B_LlaDA/.venv/lib/python3.12/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py\", line 137, in get_requires_for_build_wheel\n",
            "  \u001b[31m   \u001b[0m     backend = _build_backend()\n",
            "  \u001b[31m   \u001b[0m               ^^^^^^^^^^^^^^^^\n",
            "  \u001b[31m   \u001b[0m   File \"/Users/davidbong/Documents/AI_labs/DiffusionModel/Dream7B_LlaDA/.venv/lib/python3.12/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py\", line 70, in _build_backend\n",
            "  \u001b[31m   \u001b[0m     obj = import_module(mod_path)\n",
            "  \u001b[31m   \u001b[0m           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  \u001b[31m   \u001b[0m   File \"/opt/homebrew/Cellar/python@3.12/3.12.12_2/Frameworks/Python.framework/Versions/3.12/lib/python3.12/importlib/__init__.py\", line 90, in import_module\n",
            "  \u001b[31m   \u001b[0m     return _bootstrap._gcd_import(name[level:], package, level)\n",
            "  \u001b[31m   \u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  \u001b[31m   \u001b[0m   File \"<frozen importlib._bootstrap>\", line 1387, in _gcd_import\n",
            "  \u001b[31m   \u001b[0m   File \"<frozen importlib._bootstrap>\", line 1360, in _find_and_load\n",
            "  \u001b[31m   \u001b[0m   File \"<frozen importlib._bootstrap>\", line 1310, in _find_and_load_unlocked\n",
            "  \u001b[31m   \u001b[0m   File \"<frozen importlib._bootstrap>\", line 488, in _call_with_frames_removed\n",
            "  \u001b[31m   \u001b[0m   File \"<frozen importlib._bootstrap>\", line 1387, in _gcd_import\n",
            "  \u001b[31m   \u001b[0m   File \"<frozen importlib._bootstrap>\", line 1360, in _find_and_load\n",
            "  \u001b[31m   \u001b[0m   File \"<frozen importlib._bootstrap>\", line 1331, in _find_and_load_unlocked\n",
            "  \u001b[31m   \u001b[0m   File \"<frozen importlib._bootstrap>\", line 935, in _load_unlocked\n",
            "  \u001b[31m   \u001b[0m   File \"<frozen importlib._bootstrap_external>\", line 999, in exec_module\n",
            "  \u001b[31m   \u001b[0m   File \"<frozen importlib._bootstrap>\", line 488, in _call_with_frames_removed\n",
            "  \u001b[31m   \u001b[0m   File \"/private/var/folders/v8/d58dm48534j2765wcq9btnqr0000gn/T/pip-build-env-watmoua8/overlay/lib/python3.12/site-packages/setuptools/__init__.py\", line 16, in <module>\n",
            "  \u001b[31m   \u001b[0m     import setuptools.version\n",
            "  \u001b[31m   \u001b[0m   File \"/private/var/folders/v8/d58dm48534j2765wcq9btnqr0000gn/T/pip-build-env-watmoua8/overlay/lib/python3.12/site-packages/setuptools/version.py\", line 1, in <module>\n",
            "  \u001b[31m   \u001b[0m     import pkg_resources\n",
            "  \u001b[31m   \u001b[0m   File \"/private/var/folders/v8/d58dm48534j2765wcq9btnqr0000gn/T/pip-build-env-watmoua8/overlay/lib/python3.12/site-packages/pkg_resources/__init__.py\", line 2172, in <module>\n",
            "  \u001b[31m   \u001b[0m     register_finder(pkgutil.ImpImporter, find_on_path)\n",
            "  \u001b[31m   \u001b[0m                     ^^^^^^^^^^^^^^^^^^^\n",
            "  \u001b[31m   \u001b[0m AttributeError: module 'pkgutil' has no attribute 'ImpImporter'. Did you mean: 'zipimporter'?\n",
            "  \u001b[31m   \u001b[0m \u001b[31m[end of output]\u001b[0m\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "\u001b[31mERROR: Failed to build 'numpy' when getting requirements to build wheel\u001b[0m\u001b[31m\n",
            "\u001b[0mDataset({\n",
            "    features: ['text'],\n",
            "    num_rows: 500\n",
            "}) Dataset({\n",
            "    features: ['text'],\n",
            "    num_rows: 20\n",
            "})\n",
            "\n",
            "Example:\n",
            " One day, a little girl named Lily found a needle in her room. She knew it was difficult to play with it because it was sharp. Lily wanted to share the needle with her mom, so she could sew a button on her shirt.\n",
            "\n",
            "Lily went to her mom and said, \"Mom, I found this needle. Can you share it with me and sew my shirt?\" Her mom smiled and said, \"Yes, Lily, we can share the needle and fix your shirt.\"\n",
            "\n",
            "Together, they shared the needle and sewed the button on Lily's shirt. It was not difficult for them b\n"
          ]
        }
      ],
      "source": [
        "!pip uninstall numpy -y --quiet\n",
        "!pip install numpy==1.23.5 --quiet\n",
        "train_ds = load_dataset(\"roneneldan/TinyStories\", split=f\"train[:{TRAIN_EXAMPLES}]\")\n",
        "val_ds   = load_dataset(\"roneneldan/TinyStories\", split=f\"validation[:{VAL_EXAMPLES}]\")\n",
        "\n",
        "print(train_ds, val_ds)\n",
        "print(\"\\nExample:\\n\", train_ds[0][\"text\"][:500])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "96d2b180",
      "metadata": {
        "id": "96d2b180"
      },
      "source": [
        "# 2) Load TinyStories (small slice)\n",
        "\n",
        "We’ll load only a **slice** to keep it “not too big”."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a18aaf97",
      "metadata": {
        "id": "a18aaf97"
      },
      "source": [
        "# 3) Train a tokenizer from scratch (Byte-level BPE)\n",
        "\n",
        "We train a Byte-level BPE tokenizer ourselves (no pretrained tokenizer).\n",
        "\n",
        "Special tokens:\n",
        "\n",
        "- `[PAD]` padding\n",
        "- `[UNK]` unknown\n",
        "- `[BOS]` begin-of-sequence\n",
        "- `[EOS]` end-of-sequence\n",
        "- `[MASK]` the diffusion “noise” token\n",
        "- `<|user|>`, `<|assistant|>`, `<|system|>`, `<|end|>` for chat formatting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "7d378543",
      "metadata": {
        "id": "7d378543"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training tokenizer...\n",
            "\n",
            "\n",
            "\n",
            "Saved tokenizer to: tokenizer_from_scratch/tokenizer.json\n",
            "Vocab size: 3637\n"
          ]
        }
      ],
      "source": [
        "from tokenizers import Tokenizer\n",
        "from tokenizers.models import BPE\n",
        "from tokenizers.trainers import BpeTrainer\n",
        "from tokenizers.pre_tokenizers import ByteLevel\n",
        "from tokenizers.decoders import ByteLevel as ByteLevelDecoder\n",
        "from tokenizers.normalizers import NFKC\n",
        "from tokenizers.processors import TemplateProcessing\n",
        "\n",
        "SPECIAL_TOKENS = [\n",
        "    \"[PAD]\", \"[UNK]\", \"[BOS]\", \"[EOS]\", \"[MASK]\",\n",
        "    \"<|user|>\", \"<|assistant|>\", \"<|system|>\", \"<|end|>\",\n",
        "]\n",
        "\n",
        "def tokenizer_training_iterator(ds, n_examples):\n",
        "    for i in range(min(n_examples, len(ds))):\n",
        "        story = ds[i][\"text\"].strip()\n",
        "        yield f\"<|user|>\\nWrite a short story.\\n<|assistant|>\\n{story}\\n<|end|>\\n\"\n",
        "\n",
        "tokenizer = Tokenizer(BPE(unk_token=\"[UNK]\"))\n",
        "tokenizer.normalizer = NFKC()\n",
        "tokenizer.pre_tokenizer = ByteLevel(add_prefix_space=False)\n",
        "\n",
        "trainer = BpeTrainer(\n",
        "    vocab_size=VOCAB_SIZE,\n",
        "    min_frequency=2,\n",
        "    special_tokens=SPECIAL_TOKENS,\n",
        ")\n",
        "\n",
        "print(\"Training tokenizer...\")\n",
        "tokenizer.train_from_iterator(\n",
        "    tokenizer_training_iterator(train_ds, TOKENIZER_TRAIN_EXAMPLES),\n",
        "    trainer=trainer\n",
        ")\n",
        "\n",
        "bos_id = tokenizer.token_to_id(\"[BOS]\")\n",
        "eos_id = tokenizer.token_to_id(\"[EOS]\")\n",
        "tokenizer.post_processor = TemplateProcessing(\n",
        "    single=\"[BOS] $A [EOS]\",\n",
        "    special_tokens=[(\"[BOS]\", bos_id), (\"[EOS]\", eos_id)],\n",
        ")\n",
        "tokenizer.decoder = ByteLevelDecoder()\n",
        "\n",
        "TOKENIZER_DIR = \"tokenizer_from_scratch\"\n",
        "os.makedirs(TOKENIZER_DIR, exist_ok=True)\n",
        "TOKENIZER_FILE = os.path.join(TOKENIZER_DIR, \"tokenizer.json\")\n",
        "tokenizer.save(TOKENIZER_FILE)\n",
        "\n",
        "print(\"Saved tokenizer to:\", TOKENIZER_FILE)\n",
        "print(\"Vocab size:\", tokenizer.get_vocab_size())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "a8e3ac1e",
      "metadata": {
        "id": "a8e3ac1e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "PAD_ID: 0 MASK_ID: 4 BOS_ID: 2 EOS_ID: 3\n",
            "Example encoding: [2, 1264, 764, 9, 3]\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers -U --quiet\n",
        "from transformers import PreTrainedTokenizerFast\n",
        "\n",
        "hf_tokenizer = PreTrainedTokenizerFast(tokenizer_file=TOKENIZER_FILE)\n",
        "\n",
        "hf_tokenizer.pad_token  = \"[PAD]\"\n",
        "hf_tokenizer.unk_token  = \"[UNK]\"\n",
        "hf_tokenizer.bos_token  = \"[BOS]\"\n",
        "hf_tokenizer.eos_token  = \"[EOS]\"\n",
        "hf_tokenizer.mask_token = \"[MASK]\"\n",
        "\n",
        "hf_tokenizer.add_special_tokens({\n",
        "    \"additional_special_tokens\": [\"<|user|>\", \"<|assistant|>\", \"<|system|>\", \"<|end|>\"]\n",
        "})\n",
        "\n",
        "PAD_ID  = hf_tokenizer.pad_token_id\n",
        "MASK_ID = hf_tokenizer.mask_token_id\n",
        "BOS_ID  = hf_tokenizer.bos_token_id\n",
        "EOS_ID  = hf_tokenizer.eos_token_id\n",
        "\n",
        "print(\"PAD_ID:\", PAD_ID, \"MASK_ID:\", MASK_ID, \"BOS_ID:\", BOS_ID, \"EOS_ID:\", EOS_ID)\n",
        "print(\"Example encoding:\", hf_tokenizer.encode(\"Hello world!\")[:20])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "df6f6716",
      "metadata": {
        "id": "df6f6716"
      },
      "source": [
        "# 4) Build a tiny diffusion LM (Transformer) from scratch\n",
        "\n",
        "We implement a minimal **bidirectional Transformer** with:\n",
        "\n",
        "- token embeddings\n",
        "- position embeddings\n",
        "- **time-step embedding** (diffusion step `t`)\n",
        "- TransformerEncoder blocks\n",
        "- vocabulary projection head\n",
        "\n",
        "Training objective: predict original tokens **only at masked positions**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "ee050950",
      "metadata": {
        "id": "ee050950"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model parameters: 12.16M\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/v8/d58dm48534j2765wcq9btnqr0000gn/T/ipykernel_55068/1193024710.py:32: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
            "  self.encoder = nn.TransformerEncoder(enc_layer, num_layers=cfg.n_layers)\n"
          ]
        }
      ],
      "source": [
        "from dataclasses import dataclass\n",
        "\n",
        "@dataclass\n",
        "class DiffusionLMConfig:\n",
        "    vocab_size: int\n",
        "    seq_len: int # Context Block\n",
        "    d_model: int # Head Dimension\n",
        "    n_layers: int\n",
        "    n_heads: int\n",
        "    d_ff: int #Feed Forward \n",
        "    dropout: float\n",
        "    diffusion_steps: int # Diffusion Steps\n",
        "\n",
        "class DiffusionTransformerLM(nn.Module):\n",
        "    def __init__(self, cfg: DiffusionLMConfig):\n",
        "        super().__init__()\n",
        "        self.cfg = cfg\n",
        "\n",
        "        self.tok_emb = nn.Embedding(cfg.vocab_size, cfg.d_model)\n",
        "        self.pos_emb = nn.Embedding(cfg.seq_len, cfg.d_model)\n",
        "        self.time_emb = nn.Embedding(cfg.diffusion_steps + 1, cfg.d_model)\n",
        "\n",
        "        enc_layer = nn.TransformerEncoderLayer(\n",
        "            d_model=cfg.d_model,\n",
        "            nhead=cfg.n_heads,\n",
        "            dim_feedforward=cfg.d_ff,\n",
        "            dropout=cfg.dropout,\n",
        "            batch_first=True,\n",
        "            activation=\"gelu\",\n",
        "            norm_first=True,\n",
        "        )\n",
        "        self.encoder = nn.TransformerEncoder(enc_layer, num_layers=cfg.n_layers)\n",
        "        self.ln_f = nn.LayerNorm(cfg.d_model)\n",
        "        self.lm_head = nn.Linear(cfg.d_model, cfg.vocab_size, bias=False)\n",
        "\n",
        "        # Tie weights (optional; common in LMs) # Weights Sharing\n",
        "        self.lm_head.weight = self.tok_emb.weight\n",
        "\n",
        "        self.drop = nn.Dropout(cfg.dropout)\n",
        "\n",
        "    def forward(self, input_ids, timesteps, attention_mask=None):\n",
        "        # input_ids: [B, L]\n",
        "        # timesteps: [B] integer diffusion step in [1..T]\n",
        "        # attention_mask: [B, L] bool, True for non-pad tokens\n",
        "\n",
        "        B, L = input_ids.shape\n",
        "        if L > self.cfg.seq_len:\n",
        "            raise ValueError(f\"Sequence length {L} > cfg.seq_len {self.cfg.seq_len}\")\n",
        "\n",
        "        pos = torch.arange(L, device=input_ids.device).unsqueeze(0)  # [1, L]\n",
        "        x = self.tok_emb(input_ids) + self.pos_emb(pos)\n",
        "\n",
        "        t_emb = self.time_emb(timesteps).unsqueeze(1)  # [B, 1, D]\n",
        "        x = x + t_emb\n",
        "        x = self.drop(x)\n",
        "\n",
        "        if attention_mask is None:\n",
        "            src_key_padding_mask = None\n",
        "        else:\n",
        "            src_key_padding_mask = ~attention_mask  # invert: True = pad/ignore\n",
        "\n",
        "        x = self.encoder(x, src_key_padding_mask=src_key_padding_mask)\n",
        "        x = self.ln_f(x)\n",
        "        logits = self.lm_head(x)  # [B, L, V]\n",
        "        return logits\n",
        "\n",
        "cfg = DiffusionLMConfig(\n",
        "    vocab_size=len(hf_tokenizer),\n",
        "    seq_len=SEQ_LEN,\n",
        "    d_model=D_MODEL,\n",
        "    n_layers=N_LAYERS,\n",
        "    n_heads=N_HEADS,\n",
        "    d_ff=D_FF,\n",
        "    dropout=0.1,\n",
        "    diffusion_steps=DIFFUSION_STEPS,\n",
        ")\n",
        "model = DiffusionTransformerLM(cfg)\n",
        "\n",
        "n_params = sum(p.numel() for p in model.parameters())\n",
        "print(f\"Model parameters: {n_params/1e6:.2f}M\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "74aa03d6",
      "metadata": {
        "id": "74aa03d6"
      },
      "source": [
        "# 5) Create a token-block dataset (for language modeling)\n",
        "\n",
        "We convert each story into a chat-like format:\n",
        "\n",
        "```\n",
        "<|user|>\n",
        "Write a short story.\n",
        "<|assistant|>\n",
        "{story}\n",
        "<|end|>\n",
        "```\n",
        "\n",
        "Then we tokenize and stream tokens into fixed-length blocks (`SEQ_LEN`)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "3092a275",
      "metadata": {
        "id": "3092a275"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'input_ids': torch.Size([32, 256]), 'attention_mask': torch.Size([32, 256])}\n",
            "Decoded snippet:\n",
            " [BOS]<|user|>\n",
            "Write a short story.\n",
            "<|assistant|>\n",
            "Once there was a lady who had a special leg. It was deaf and she never wanted to talk about it.\n",
            "\n",
            "One day, a little boy asked the lady why her leg was so special. She was surprised he noticed!\n",
            "\n",
            "The lady smiled and told the boy she was deaf in the leg because she was born that way.\n",
            "\n",
            "The boy was surprisedâ€” he thought it was unusual but very cool! He asked the lady to teach him sign language and she gladly agreed!\n",
            "\n",
            "The lady\n"
          ]
        }
      ],
      "source": [
        "def format_as_chat(story_text: str) -> str:\n",
        "    story_text = story_text.strip()\n",
        "    return f\"<|user|>\\nWrite a short story.\\n<|assistant|>\\n{story_text}\\n<|end|>\\n\"\n",
        "\n",
        "class TokenBlockDataset(IterableDataset):\n",
        "    def __init__(self, hf_ds, tokenizer, seq_len, shuffle=False, seed=0):\n",
        "        self.hf_ds = hf_ds\n",
        "        self.tokenizer = tokenizer\n",
        "        self.seq_len = seq_len\n",
        "        self.shuffle = shuffle\n",
        "        self.seed = seed\n",
        "\n",
        "    def __iter__(self):\n",
        "        indices = list(range(len(self.hf_ds)))\n",
        "        if self.shuffle:\n",
        "            rng = random.Random(self.seed)\n",
        "            rng.shuffle(indices)\n",
        "\n",
        "        buffer = []\n",
        "        for idx in indices:\n",
        "            text = format_as_chat(self.hf_ds[idx][\"text\"])\n",
        "            ids = self.tokenizer.encode(text, add_special_tokens=True)\n",
        "            buffer.extend(ids)\n",
        "\n",
        "            while len(buffer) >= self.seq_len:\n",
        "                block = buffer[:self.seq_len]\n",
        "                buffer = buffer[self.seq_len:]\n",
        "                yield torch.tensor(block, dtype=torch.long)\n",
        "\n",
        "train_blocks = TokenBlockDataset(train_ds, hf_tokenizer, SEQ_LEN, shuffle=True, seed=42)\n",
        "val_blocks   = TokenBlockDataset(val_ds,   hf_tokenizer, SEQ_LEN, shuffle=False)\n",
        "\n",
        "def collate_blocks(batch):\n",
        "    input_ids = torch.stack(batch, dim=0)  # [B, L]\n",
        "    attention_mask = (input_ids != PAD_ID)\n",
        "    return {\"input_ids\": input_ids, \"attention_mask\": attention_mask}\n",
        "\n",
        "train_loader = DataLoader(train_blocks, batch_size=BATCH_SIZE, collate_fn=collate_blocks)\n",
        "val_loader   = DataLoader(val_blocks,   batch_size=BATCH_SIZE, collate_fn=collate_blocks)\n",
        "\n",
        "b = next(iter(train_loader))\n",
        "print({k: v.shape for k, v in b.items()})\n",
        "print(\"Decoded snippet:\\n\", hf_tokenizer.decode(b[\"input_ids\"][0][:120].tolist()))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7810604f",
      "metadata": {
        "id": "7810604f"
      },
      "source": [
        "# 6) Diffusion corruption (masking) + training loss\n",
        "\n",
        "For each batch:\n",
        "\n",
        "1. Sample diffusion step `t ∈ {1…T}` for each sequence.\n",
        "2. Corrupt clean `x₀` into `x_t` by replacing a fraction of tokens with `[MASK]`.\n",
        "3. Train the model to predict original IDs **only at masked positions**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "990df483",
      "metadata": {
        "id": "990df483"
      },
      "outputs": [],
      "source": [
        "def mask_ratio_schedule(t, T: int):\n",
        "    # Linear schedule: ratio = t/T\n",
        "    return t.float() / float(T)\n",
        "\n",
        "@torch.no_grad()\n",
        "def corrupt_with_mask(input_ids, attention_mask, t, mask_token_id: int, T: int):\n",
        "    # Returns noisy_ids, labels, mask_positions\n",
        "    B, L = input_ids.shape\n",
        "    ratio = mask_ratio_schedule(t, T).unsqueeze(1)  # [B,1]\n",
        "\n",
        "    can_mask = attention_mask.clone()\n",
        "    can_mask &= (input_ids != BOS_ID) & (input_ids != EOS_ID) & (input_ids != PAD_ID)\n",
        "\n",
        "    rand = torch.rand((B, L), device=input_ids.device)\n",
        "    mask_positions = (rand < ratio) & can_mask\n",
        "\n",
        "    noisy = input_ids.clone()\n",
        "    noisy[mask_positions] = mask_token_id\n",
        "\n",
        "    labels = torch.full_like(input_ids, -100)\n",
        "    labels[mask_positions] = input_ids[mask_positions]\n",
        "\n",
        "    return noisy, labels, mask_positions\n",
        "\n",
        "def diffusion_loss(model, batch, T: int):\n",
        "    input_ids = batch[\"input_ids\"]\n",
        "    attention_mask = batch[\"attention_mask\"]\n",
        "\n",
        "    B = input_ids.size(0)\n",
        "    t = torch.randint(1, T + 1, (B,), device=input_ids.device)\n",
        "\n",
        "    noisy_ids, labels, _ = corrupt_with_mask(\n",
        "        input_ids=input_ids,\n",
        "        attention_mask=attention_mask,\n",
        "        t=t,\n",
        "        mask_token_id=MASK_ID,\n",
        "        T=T,\n",
        "    )\n",
        "\n",
        "    logits = model(noisy_ids, timesteps=t, attention_mask=attention_mask)  # [B,L,V]\n",
        "    loss = F.cross_entropy(\n",
        "        logits.view(-1, logits.size(-1)),\n",
        "        labels.view(-1),\n",
        "        ignore_index=-100,\n",
        "    )\n",
        "    return loss"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6b78c7e6",
      "metadata": {
        "id": "6b78c7e6"
      },
      "source": [
        "# 7) Train (from scratch)\n",
        "\n",
        "By default this notebook saves **only the final checkpoint**:\n",
        "\n",
        "- `checkpoints/final/model.pt`\n",
        "- `checkpoints/final/config.json`\n",
        "- `checkpoints/final/tokenizer/`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "ed9a07a4",
      "metadata": {
        "id": "ed9a07a4"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d142e1632b7a4c0480a981b0ba2c7d45",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/200 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved final checkpoint to: checkpoints/final\n"
          ]
        }
      ],
      "source": [
        "from accelerate import Accelerator\n",
        "from transformers import get_cosine_schedule_with_warmup\n",
        "\n",
        "accelerator = Accelerator(mixed_precision=\"bf16\" if torch.cuda.is_available() and torch.cuda.is_bf16_supported() else \"fp16\")\n",
        "device = accelerator.device\n",
        "\n",
        "model = model.to(device)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n",
        "\n",
        "scheduler = get_cosine_schedule_with_warmup(\n",
        "    optimizer,\n",
        "    num_warmup_steps=WARMUP_STEPS,\n",
        "    num_training_steps=TRAIN_STEPS,\n",
        ")\n",
        "\n",
        "model, optimizer, train_loader, val_loader, scheduler = accelerator.prepare(\n",
        "    model, optimizer, train_loader, val_loader, scheduler\n",
        ")\n",
        "\n",
        "def eval_loss(n_batches=20):\n",
        "    model.eval()\n",
        "    losses = []\n",
        "    with torch.no_grad():\n",
        "        for i, batch in enumerate(val_loader):\n",
        "            if i >= n_batches:\n",
        "                break\n",
        "\n",
        "            loss = diffusion_loss(model, batch, T=cfg.diffusion_steps)\n",
        "\n",
        "            # gather across processes -> always make it 1D\n",
        "            gathered = accelerator.gather(loss.detach().float().reshape(1))\n",
        "\n",
        "            # now gathered is shape [world_size] (or [1] on single GPU)\n",
        "            losses.append(gathered.cpu())\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    if len(losses) == 0:\n",
        "        return float(\"nan\")\n",
        "\n",
        "    losses = torch.cat(losses)   # safe: all are 1D tensors\n",
        "    return losses.mean().item()\n",
        "\n",
        "\n",
        "model.train()\n",
        "pbar = tqdm(range(TRAIN_STEPS), disable=not accelerator.is_main_process)\n",
        "running = []\n",
        "\n",
        "train_iter = iter(train_loader)\n",
        "\n",
        "for step in pbar:\n",
        "    try:\n",
        "        batch = next(train_iter)\n",
        "    except StopIteration:\n",
        "        train_iter = iter(train_loader)\n",
        "        batch = next(train_iter)\n",
        "\n",
        "    loss = diffusion_loss(model, batch, T=cfg.diffusion_steps) / GRAD_ACCUM\n",
        "    accelerator.backward(loss)\n",
        "\n",
        "    if (step + 1) % GRAD_ACCUM == 0:\n",
        "        accelerator.clip_grad_norm_(model.parameters(), 1.0)\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "    running.append(loss.item() * GRAD_ACCUM)\n",
        "\n",
        "    if (step + 1) % 50 == 0 and accelerator.is_main_process:\n",
        "        pbar.set_description(f\"loss={np.mean(running[-50:]):.4f} lr={scheduler.get_last_lr()[0]:.2e}\")\n",
        "\n",
        "    if (step + 1) % 500 == 0 and accelerator.is_main_process:\n",
        "        val_l = eval_loss(n_batches=10)\n",
        "        print(f\"\\nStep {step+1} | val_loss ~ {val_l:.4f}\")\n",
        "\n",
        "if accelerator.is_main_process:\n",
        "    OUT_DIR = \"checkpoints/final\"\n",
        "    os.makedirs(OUT_DIR, exist_ok=True)\n",
        "    torch.save(accelerator.unwrap_model(model).state_dict(), os.path.join(OUT_DIR, \"model.pt\"))\n",
        "    with open(os.path.join(OUT_DIR, \"config.json\"), \"w\") as f:\n",
        "        json.dump(cfg.__dict__, f, indent=2)\n",
        "    hf_tokenizer.save_pretrained(os.path.join(OUT_DIR, \"tokenizer\"))\n",
        "    print(\"Saved final checkpoint to:\", OUT_DIR)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "718c9883",
      "metadata": {
        "id": "718c9883"
      },
      "source": [
        "# 8) Diffusion sampling (progressive unmasking)\n",
        "\n",
        "Prompt tokens are **fixed**. The “assistant answer” region starts fully masked.\n",
        "\n",
        "At each step we:\n",
        "- predict tokens for masked positions\n",
        "- keep the most confident tokens\n",
        "- re-mask the least confident tokens\n",
        "\n",
        "We record intermediate steps for a GIF."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "67ca6dd8",
      "metadata": {
        "id": "67ca6dd8"
      },
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def diffusion_generate(\n",
        "    model,\n",
        "    tokenizer,\n",
        "    prompt_text: str,\n",
        "    max_new_tokens: int = 128,\n",
        "    diffusion_steps: int = 64,\n",
        "    temperature: float = 1.0,\n",
        "    top_k: int = 0,\n",
        "    record_steps: bool = True,\n",
        "):\n",
        "    model.eval()\n",
        "    device = next(model.parameters()).device\n",
        "\n",
        "    prompt_ids = tokenizer.encode(prompt_text, add_special_tokens=True)\n",
        "    prompt_ids = torch.tensor(prompt_ids, dtype=torch.long, device=device).unsqueeze(0)  # [1, Lp]\n",
        "\n",
        "    Lp = prompt_ids.size(1)\n",
        "    L = min(cfg.seq_len, Lp + max_new_tokens)\n",
        "    gen_len = L - Lp\n",
        "\n",
        "    x = torch.full((1, L), MASK_ID, dtype=torch.long, device=device)\n",
        "    x[:, :Lp] = prompt_ids[:, :Lp]\n",
        "\n",
        "    fixed = torch.zeros((1, L), dtype=torch.bool, device=device)\n",
        "    fixed[:, :Lp] = True\n",
        "\n",
        "    attention_mask = torch.ones((1, L), dtype=torch.bool, device=device)\n",
        "\n",
        "    frames = []\n",
        "    \n",
        "    def sample_from_logits(logits):\n",
        "        if temperature != 1.0:\n",
        "            logits = logits / temperature\n",
        "\n",
        "        if top_k and top_k > 0:\n",
        "            topk_vals, topk_idx = torch.topk(logits, k=top_k, dim=-1)\n",
        "            filtered = torch.full_like(logits, float(\"-inf\"))\n",
        "            filtered.scatter_(-1, topk_idx, topk_vals)\n",
        "            logits = filtered\n",
        "\n",
        "        probs = F.softmax(logits, dim=-1)\n",
        "        flat = probs.view(-1, probs.size(-1))\n",
        "        sampled = torch.multinomial(flat, num_samples=1).view(1, L)\n",
        "        sampled_prob = probs.gather(-1, sampled.unsqueeze(-1)).squeeze(-1)  # [1,L]\n",
        "        return sampled, sampled_prob\n",
        "\n",
        "    for s in range(diffusion_steps, 0, -1):\n",
        "        t = torch.tensor([s], device=device, dtype=torch.long)\n",
        "        logits = model(x, timesteps=t, attention_mask=attention_mask)\n",
        "        sampled, conf = sample_from_logits(logits)\n",
        "\n",
        "        update_pos = ~fixed\n",
        "        x[update_pos] = sampled[update_pos]\n",
        "\n",
        "        next_ratio = float(s - 1) / float(diffusion_steps)\n",
        "        target_masks = int(math.ceil(gen_len * next_ratio))\n",
        "\n",
        "        gen_positions = torch.arange(L, device=device) >= Lp\n",
        "        candidates = gen_positions & (~fixed[0])\n",
        "        cand_idx = torch.where(candidates)[0]\n",
        "\n",
        "        if target_masks > 0 and cand_idx.numel() > 0:\n",
        "            cand_conf = conf[0, cand_idx]\n",
        "            k = min(target_masks, cand_idx.numel())\n",
        "            _, low_idx = torch.topk(cand_conf, k=k, largest=False)\n",
        "            remask_positions = cand_idx[low_idx]\n",
        "            x[0, remask_positions] = MASK_ID\n",
        "\n",
        "        if record_steps:\n",
        "            decoded = tokenizer.decode(x[0].tolist())\n",
        "            decoded = decoded.replace(\"[MASK]\", \"█\")\n",
        "            frames.append((s, decoded))\n",
        "\n",
        "    final = tokenizer.decode(x[0].tolist())\n",
        "    model.train()\n",
        "    return final, frames\n",
        "\n",
        "def chat_prompt(user_msg: str, system_msg: str = None) -> str:\n",
        "    parts = []\n",
        "    if system_msg:\n",
        "        parts.append(f\"<|system|>\\n{system_msg}\\n\")\n",
        "    parts.append(f\"<|user|>\\n{user_msg}\\n\")\n",
        "    parts.append(\"<|assistant|>\\n\")\n",
        "    return \"\".join(parts)\n",
        "\n",
        "TEST_USER_PROMPT = \"Once upon a time\"\n",
        "prompt_text = chat_prompt(TEST_USER_PROMPT)\n",
        "\n",
        "final_text, frames = diffusion_generate(\n",
        "    model=accelerator.unwrap_model(model),\n",
        "    tokenizer=hf_tokenizer,\n",
        "    prompt_text=prompt_text,\n",
        "    max_new_tokens=128,\n",
        "    diffusion_steps=cfg.diffusion_steps,\n",
        "    temperature=1.0,\n",
        "    top_k=50,\n",
        "    record_steps=True,\n",
        ")\n",
        "\n",
        "print(\"Final decoded (raw):\\n\")\n",
        "print(final_text[:1000])\n",
        "print(\"\\nRecorded frames:\", len(frames))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
