{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "58676361",
   "metadata": {},
   "outputs": [],
   "source": [
    "def q_sample_masked(x_0, t, T, mask_token_id):\n",
    "    \"\"\"\n",
    "    Discrete diffusion process for LlaDA.\n",
    "    Progressively masks tokens based on timestep t.\n",
    "    \"\"\"\n",
    "    # Calculate masking ratio (linearly increasing)\n",
    "    mask_ratio = t / T\n",
    "    \n",
    "    # Create a mask based on the ratio\n",
    "    mask = torch.rand_like(x_0.float()) < mask_ratio\n",
    "    \n",
    "    # Apply mask: Replace tokens with mask_token_id\n",
    "    x_t = x_0.clone()\n",
    "    x_t[mask] = mask_token_id\n",
    "    \n",
    "    return x_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4a23353c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def calculate_sequence_probability(model, input_ids):\n",
    "    \"\"\"\n",
    "    Implements p(x) = p(x1) * product_{n=2}^N p(xn | x1:n-1)\n",
    "    In practice, we use log-probabilities to avoid underflow:\n",
    "    log p(x) = sum_{n=1}^N log p(xn | x1:n-1)\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # 1. Forward pass to get logits for all positions\n",
    "        outputs = model(input_ids)\n",
    "        logits = outputs.logits # Shape: (batch, seq_len, vocab_size)\n",
    "        \n",
    "        # 2. Shift logits and targets to align p(xn | x_{1:n-1})\n",
    "        # Logits at index i predict token at index i+1\n",
    "        shift_logits = logits[:, :-1, :].contiguous()\n",
    "        shift_labels = input_ids[:, 1:].contiguous()\n",
    "        \n",
    "        # 3. Calculate log p(xn | history) using CrossEntropy\n",
    "        # CrossEntropyLoss(reduction='none') returns -log p for each token\n",
    "        log_probs = -F.cross_entropy(\n",
    "            shift_logits.view(-1, shift_logits.size(-1)), \n",
    "            shift_labels.view(-1), \n",
    "            reduction='none'\n",
    "        )\n",
    "        \n",
    "        # 4. Add the probability of the first token p(x1)\n",
    "        # Usually assumed to be 1.0 (0.0 log) if the sequence starts with BOS\n",
    "        # Or calculated via a start-of-sequence distribution.\n",
    "        \n",
    "        # Total sequence log-probability (Sum of logs = Log of product)\n",
    "        total_log_prob = log_probs.sum()\n",
    "        \n",
    "        return total_log_prob.exp(), total_log_prob\n",
    "\n",
    "def autoregressive_generation(model, tokenizer, prompt, max_n=50):\n",
    "    \"\"\"\n",
    "    Demonstrates the iterative application of p(xn | x1:n-1)\n",
    "    to generate a sequence.\n",
    "    \"\"\"\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "    \n",
    "    for _ in range(max_n):\n",
    "        # Current sequence: x1:n-1\n",
    "        outputs = model(input_ids)\n",
    "        \n",
    "        # Get logits for the very last token: p(xn | x1:n-1)\n",
    "        next_token_logits = outputs.logits[:, -1, :]\n",
    "        \n",
    "        # Sample or take Argmax to get xn\n",
    "        next_token = torch.argmax(next_token_logits, dim=-1).unsqueeze(0)\n",
    "        \n",
    "        # Append xn to history: x1:n = [x1:n-1, xn]\n",
    "        input_ids = torch.cat([input_ids, next_token], dim=-1)\n",
    "        \n",
    "        if next_token == tokenizer.eos_token_id:\n",
    "            break\n",
    "            \n",
    "    return tokenizer.decode(input_ids[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5ca29397",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def diffusion_loss_fn(model, x_0, mask_token_id):\n",
    "    \"\"\"\n",
    "    Implements Equation (3):\n",
    "    L(θ) = -E [ w(t) * sum_{n=1}^N 1_{xt_n=MASK} * log pθ(x0_n | xt) ]\n",
    "    \"\"\"\n",
    "    batch_size, seq_len = x_0.shape\n",
    "    device = x_0.device\n",
    "\n",
    "    # 1. Sample t ~ U(0, 1)\n",
    "    t = torch.rand(batch_size, device=device)\n",
    "\n",
    "    # 2. Generate xt ~ q(xt | x0) \n",
    "    # For α_t = 1 - t, we mask tokens with probability t\n",
    "    noise_probs = t.view(-1, 1).expand(batch_size, seq_len)\n",
    "    mask_indices = torch.bernoulli(noise_probs).bool()\n",
    "    \n",
    "    x_t = x_0.clone()\n",
    "    x_t[mask_indices] = mask_token_id\n",
    "\n",
    "    # 3. Get model predictions pθ(x0 | xt)\n",
    "    outputs = model(x_t)\n",
    "    logits = outputs.logits  # Shape: (batch, seq_len, vocab_size)\n",
    "\n",
    "    # 4. Calculate Cross-Entropy Loss\n",
    "    # Flatten for F.cross_entropy\n",
    "    ce_loss = F.cross_entropy(\n",
    "        logits.view(-1, logits.size(-1)), \n",
    "        x_0.view(-1), \n",
    "        reduction='none'\n",
    "    ).view(batch_size, seq_len)\n",
    "\n",
    "    # 5. Apply Indicator Function: 1_{xt_n=MASK}\n",
    "    # Only compute loss for positions that were masked\n",
    "    masked_loss = ce_loss * mask_indices.float()\n",
    "\n",
    "    # 6. Apply Reweighting Term: w(t) = 1/t\n",
    "    # We use a small epsilon to avoid division by zero\n",
    "    w_t = 1.0 / torch.clamp(t, min=1e-5)\n",
    "    \n",
    "    # Final Weighted Loss (Sum over N, Mean over Batch)\n",
    "    loss = (w_t.view(-1, 1) * masked_loss).sum(dim=1).mean()\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "283c9c63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Shape: torch.Size([2, 4])\n",
      "Flattened (view): torch.Size([8])\n",
      "Flattened (view): tensor([10, 20, 30, 40, 50, 60, 70, 80])\n",
      "Reshaped (4x2): torch.Size([4, 2])\n",
      "Reshaped (4x2): tensor([[10, 20],\n",
      "        [30, 40],\n",
      "        [50, 60],\n",
      "        [70, 80]])\n",
      "Unsqueezed at 0: torch.Size([1, 2, 4])\n",
      "Unsqueezed at 0: tensor([[[10, 20, 30, 40],\n",
      "         [50, 60, 70, 80]]])\n",
      "Unsqueezed at 1: torch.Size([2, 1, 4])\n",
      "Unsqueezed at 1: tensor([[[10, 20, 30, 40]],\n",
      "\n",
      "        [[50, 60, 70, 80]]])\n",
      "\n",
      "Complex Tensor y: torch.Size([1, 2, 1, 4])\n",
      "\n",
      "Complex Tensor y: tensor([[[[ 0.4877,  0.9192,  0.0088,  0.8201]],\n",
      "\n",
      "         [[ 0.8905,  0.6694, -1.3324, -0.0963]]]])\n",
      "Fully Squeezed: torch.Size([2, 4])\n",
      "Fully Squeezed: tensor([[ 0.4877,  0.9192,  0.0088,  0.8201],\n",
      "        [ 0.8905,  0.6694, -1.3324, -0.0963]])\n",
      "Squeezed at index 0: torch.Size([2, 1, 4])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Create a sample tensor (e.g., 2 sentences, 4 tokens each)\n",
    "# Shape: (Batch=2, Seq_Len=4)\n",
    "x = torch.tensor([[10, 20, 30, 40], [50, 60, 70, 80]])\n",
    "print(f\"Original Shape: {x.shape}\")\n",
    "\n",
    "# --- VIEW ---\n",
    "# Flatten the tensor into one long sequence\n",
    "# Shape: (8)\n",
    "flattened = x.view(8) \n",
    "# OR use -1 to auto-calculate: x.view(-1)\n",
    "print(f\"Flattened (view): {flattened.shape}\")\n",
    "print(f\"Flattened (view): {flattened}\")\n",
    "\n",
    "# Reshape into (4, 2)\n",
    "reshaped = x.view(4, 2)\n",
    "print(f\"Reshaped (4x2): {reshaped.shape}\")\n",
    "print(f\"Reshaped (4x2): {reshaped}\")\n",
    "\n",
    "# --- UNSQUEEZE (Adding Dimensions) ---\n",
    "# Add a dimension at the start (often needed for model input)\n",
    "# Shape: (1, 2, 4) -> (Extra_Dim, Batch, Seq_Len)\n",
    "unsqueezed = x.unsqueeze(0)\n",
    "print(f\"Unsqueezed at 0: {unsqueezed.shape}\")\n",
    "print(f\"Unsqueezed at 0: {unsqueezed}\")\n",
    "\n",
    "# Add a dimension in the middle\n",
    "# Shape: (2, 1, 4)\n",
    "middle_dim = x.unsqueeze(1)\n",
    "print(f\"Unsqueezed at 1: {middle_dim.shape}\")\n",
    "print(f\"Unsqueezed at 1: {middle_dim}\")\n",
    "\n",
    "# --- SQUEEZE (Removing Dimensions) ---\n",
    "# Create a tensor with many 'empty' dimensions\n",
    "# Shape: (1, 2, 1, 4)\n",
    "y = torch.randn(1, 2, 1, 4)\n",
    "print(f\"\\nComplex Tensor y: {y.shape}\")\n",
    "print(f\"\\nComplex Tensor y: {y}\")\n",
    "# Remove all size-1 dimensions\n",
    "# Shape: (2, 4)\n",
    "y_simple = y.squeeze()\n",
    "print(f\"Fully Squeezed: {y_simple.shape}\")\n",
    "print(f\"Fully Squeezed: {y_simple}\")\n",
    "# Remove only the dimension at index 0\n",
    "# Shape: (2, 1, 4)\n",
    "y_specific = y.squeeze(0)\n",
    "print(f\"Squeezed at index 0: {y_specific.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e4cdea46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Shape:  tensor([[  1,  42,   5,   9],\n",
      "        [ 10,   3, 999,   0]])\n",
      "Output Shape: tensor([[[ 4.1667e-01,  9.6009e-01, -3.7386e-01,  9.4238e-01, -6.1474e-01,\n",
      "          -8.7216e-01,  6.3071e-01,  1.5478e+00,  3.3835e-01, -4.1710e-01,\n",
      "          -4.6828e-02,  9.9981e-01, -6.6385e-01,  5.2313e-01,  6.0928e-01,\n",
      "          -2.2387e-01, -1.0923e+00,  1.8785e+00, -8.2437e-01,  1.3850e-01,\n",
      "           8.7227e-01,  9.0803e-01,  7.3128e-03,  7.4487e-01,  6.6348e-02,\n",
      "          -1.6220e-02, -1.5093e+00, -1.9420e+00,  7.0590e-01, -9.9569e-01,\n",
      "           2.8487e-01, -1.0313e+00,  1.1899e+00,  6.2449e-01,  6.7444e-01,\n",
      "          -7.3596e-01,  9.8426e-01,  5.4845e-01, -1.0978e+00,  5.0840e-01,\n",
      "          -1.0143e+00, -8.4057e-01,  1.1627e+00,  1.0908e+00,  1.0844e+00,\n",
      "          -3.8818e-01,  6.9291e-01, -1.1719e+00,  1.2504e+00,  1.0046e+00,\n",
      "          -2.8554e-01,  8.5279e-01, -2.5443e-03,  7.3911e-02, -2.0046e-01,\n",
      "          -1.0361e+00, -6.4444e-01,  6.6033e-01,  1.6177e+00, -7.6691e-01,\n",
      "          -8.1469e-01, -1.7578e+00, -1.5975e-01,  7.0958e-01,  1.2737e+00,\n",
      "          -1.8346e+00,  6.6625e-02,  1.2401e+00,  3.2737e-01, -2.1677e-01,\n",
      "          -1.1465e-01, -4.4193e-01, -2.1761e+00, -1.9383e+00,  5.9119e-01,\n",
      "           1.4546e-01,  1.2181e+00, -9.2102e-01, -2.6065e-01,  6.8383e-01,\n",
      "           8.3241e-01, -7.1587e-01, -9.2103e-01, -8.8401e-01,  1.2913e-01,\n",
      "           1.0214e-01,  1.3991e+00, -1.2864e+00,  1.0596e+00, -6.7033e-01,\n",
      "          -5.6568e-01,  1.3914e-01, -1.3808e+00, -3.2585e-01, -4.3036e-01,\n",
      "          -1.1837e+00,  7.2690e-01,  4.3884e-01,  9.7401e-01,  1.2271e+00],\n",
      "         [ 5.3519e-01, -1.3224e+00,  2.1067e+00,  6.7028e-01, -1.4340e+00,\n",
      "          -3.6025e-02, -1.1952e+00, -3.1859e-01, -5.0761e-01,  1.1893e+00,\n",
      "          -2.7857e+00, -2.2651e-01, -1.3067e+00, -1.0506e+00, -5.1584e-01,\n",
      "          -4.4649e-01, -6.2991e-01, -4.4128e-01,  1.9368e-01, -1.5509e+00,\n",
      "           4.9211e-01, -2.3399e-01,  1.2904e-01,  1.3293e+00,  1.1460e+00,\n",
      "           7.0560e-01,  1.4966e+00,  8.0516e-01,  2.2169e-01, -9.4143e-01,\n",
      "           1.4411e+00, -4.6111e-03, -3.7571e-01,  1.6756e-01,  1.2909e+00,\n",
      "          -6.4294e-02, -2.1317e-01,  1.6070e-01,  1.5726e+00,  2.3768e-01,\n",
      "          -9.9531e-01, -7.4502e-01,  7.7928e-01, -1.2536e+00, -5.0201e-01,\n",
      "          -2.0117e+00, -6.1772e-01,  6.8732e-01, -9.8447e-01,  1.6385e+00,\n",
      "          -2.3641e-01,  3.2927e-01,  3.8622e-01, -8.2426e-01, -1.6656e-01,\n",
      "           1.4217e-01, -1.3270e+00, -4.3164e-01, -5.4505e-01, -6.7179e-01,\n",
      "           4.2112e-01, -4.3821e-01, -7.8107e-01, -2.9380e-01,  1.6586e+00,\n",
      "          -2.2051e+00, -4.0131e-01,  3.1359e-01,  2.6331e-01, -2.0201e-01,\n",
      "          -1.9556e+00,  3.5840e-01, -1.3234e+00,  1.0253e+00, -1.5658e+00,\n",
      "           9.5933e-01,  6.8183e-01,  6.7008e-01,  1.3838e+00, -1.3338e+00,\n",
      "          -1.0156e-01, -1.3692e+00,  1.2296e+00, -1.2900e-01,  5.9751e-01,\n",
      "          -4.3702e-01, -6.1847e-01, -6.3455e-01, -9.4463e-01,  8.5301e-01,\n",
      "           4.4469e-01, -8.4155e-01, -7.4131e-01, -5.7431e-02, -9.0955e-01,\n",
      "          -1.0533e+00,  1.2808e+00, -1.8332e-01,  2.9754e+00,  2.1980e+00],\n",
      "         [-1.7718e+00,  8.3943e-01, -1.0385e+00, -8.1541e-02, -2.7159e-02,\n",
      "          -5.4902e-01,  1.5394e+00, -6.6805e-01, -1.5139e-02,  2.0111e+00,\n",
      "           1.0597e+00, -2.5992e+00,  1.6326e-01, -2.0762e+00,  6.9123e-01,\n",
      "          -1.9651e+00, -6.7801e-01, -4.4909e-01,  2.2848e-01, -1.0249e+00,\n",
      "           1.2373e+00, -2.9705e-01, -4.6618e-01,  8.0912e-01, -1.1350e+00,\n",
      "           2.5466e-01, -3.8294e-01, -3.5561e-01,  1.9376e+00,  8.0511e-01,\n",
      "           1.1791e+00, -1.6564e+00, -3.2135e-01, -1.3003e+00,  5.2689e-01,\n",
      "           5.2411e-01,  1.3040e+00,  5.9334e-01,  2.0404e+00, -9.4620e-01,\n",
      "          -1.0458e+00,  6.9001e-01,  1.8721e-01, -9.7588e-02,  2.5285e-02,\n",
      "          -8.4898e-01, -3.6792e-01,  1.3646e+00,  3.9565e-01,  4.1037e-01,\n",
      "           3.5360e-02, -1.7030e-01,  9.2443e-02,  1.0816e+00,  7.5983e-01,\n",
      "          -8.5843e-02, -9.8026e-01,  7.2834e-01, -8.3436e-01,  1.6479e+00,\n",
      "          -7.0659e-01, -8.3912e-01, -7.9424e-02, -1.8280e-01, -1.0576e+00,\n",
      "           1.5831e+00,  1.3734e+00, -5.1979e-01, -3.2240e-01,  1.1299e+00,\n",
      "          -3.1640e+00, -7.8307e-01,  1.0561e+00, -1.3282e-01,  1.1308e+00,\n",
      "          -3.5292e-01,  1.9481e-02, -1.4446e+00, -1.7317e+00, -1.8386e+00,\n",
      "           6.5686e-01, -3.1188e-01, -1.7039e-01, -1.0494e+00,  1.0431e+00,\n",
      "          -2.0616e-01, -8.8544e-01,  6.5992e-01, -8.2502e-01, -1.1492e+00,\n",
      "           1.9145e+00,  1.8762e+00, -8.6283e-01,  2.1126e+00,  1.0838e+00,\n",
      "          -1.1122e+00,  1.1800e+00, -4.7694e-01,  7.1022e-01,  1.2853e+00],\n",
      "         [-1.4337e+00, -2.4421e-01, -1.3291e+00, -4.3375e-01, -7.6950e-01,\n",
      "          -9.7135e-02, -8.1747e-01,  1.3988e+00,  1.2369e+00,  4.7487e-01,\n",
      "          -3.9889e-01, -7.8258e-01, -1.3689e+00, -4.5157e-01,  3.4158e-01,\n",
      "           2.0663e-01,  4.7240e-01,  5.7632e-02, -7.2174e-03,  4.8727e-01,\n",
      "          -2.4639e-02,  1.6744e+00,  5.7763e-01,  9.3372e-01,  1.7048e-01,\n",
      "          -6.2440e-01, -1.1151e+00, -1.6541e+00,  1.3153e+00, -1.1600e-01,\n",
      "          -2.1379e+00, -3.5791e-01,  3.2155e-01,  1.5248e+00,  1.1798e+00,\n",
      "          -7.8729e-01,  1.5597e+00,  2.5260e-01, -1.0158e+00,  2.3665e-01,\n",
      "           7.6310e-01,  1.1989e+00,  9.5487e-01,  1.2318e+00, -2.5275e-01,\n",
      "           1.7124e+00,  7.2683e-01,  1.0612e+00, -7.6101e-01,  5.8594e-01,\n",
      "           9.9711e-01, -3.4757e-01,  1.1952e+00,  6.2405e-01,  3.4556e-01,\n",
      "          -1.5858e+00, -1.2013e+00, -3.4879e-02,  2.3304e-01, -8.1038e-02,\n",
      "           1.0095e+00,  2.0385e+00, -1.1146e+00,  2.9327e-01,  1.2782e+00,\n",
      "          -7.7873e-01,  4.3444e-02, -6.9330e-01, -3.7371e-01, -5.5504e-01,\n",
      "           5.7152e-01, -8.7132e-02,  3.8006e-01,  1.3972e+00,  1.0199e+00,\n",
      "          -1.7791e+00, -2.6742e-01,  6.2405e-01, -6.1965e-01,  4.8288e-01,\n",
      "           2.1605e+00, -1.8249e+00,  1.2479e+00,  4.0624e-01,  8.8508e-01,\n",
      "          -1.4451e+00, -1.3023e+00,  9.5432e-01, -5.0144e-01,  8.5292e-01,\n",
      "           1.5398e-01,  2.9855e-01,  6.8180e-01, -1.8129e+00, -1.2510e+00,\n",
      "           9.6022e-02, -4.6875e-01,  1.1118e+00,  8.0278e-01, -1.1077e+00]],\n",
      "\n",
      "        [[ 8.3907e-03, -1.0434e+00, -6.7989e-01, -4.1023e-01, -2.0319e-01,\n",
      "           2.8451e-01, -1.4501e+00, -9.4281e-01, -1.5667e-01, -2.2688e-01,\n",
      "           1.1874e+00, -2.6014e+00,  6.0398e-01, -1.2088e+00,  2.0802e-01,\n",
      "           3.1820e-01, -6.0901e-02,  1.0262e+00, -1.0955e-01,  3.4528e-01,\n",
      "           2.7960e-02,  5.9168e-01,  1.9499e+00,  5.9551e-01,  3.1367e-01,\n",
      "           2.9667e-01,  2.0714e+00, -4.7377e-01, -8.0210e-01, -1.2033e+00,\n",
      "          -1.3202e+00, -2.2493e+00,  6.5012e-01,  1.9162e+00,  6.4878e-01,\n",
      "          -4.9163e-01,  1.5678e+00, -3.1966e-01, -1.3283e-01,  5.1904e-02,\n",
      "           2.4734e-01, -6.3693e-01, -4.9763e-01, -5.3911e-03,  5.5511e-01,\n",
      "           1.4085e+00, -3.7507e-02, -1.8192e+00, -5.8859e-01, -8.3030e-01,\n",
      "           4.6513e-01, -1.7089e-01,  5.6473e-01, -8.5329e-01, -1.0968e+00,\n",
      "           1.7207e-01, -8.4507e-01, -1.0572e+00,  3.9417e-01, -4.0054e-01,\n",
      "          -4.9582e-01, -8.0016e-01,  5.5586e-02, -1.4760e+00,  4.4513e-02,\n",
      "           1.7110e+00,  1.8171e+00, -2.5423e-01, -4.6637e-01, -8.5178e-01,\n",
      "           1.2872e+00,  2.9793e+00,  4.0596e-01, -2.7278e+00, -5.9708e-01,\n",
      "           1.1611e-01, -9.3776e-01,  6.2383e-01, -3.5741e-01,  4.0901e-01,\n",
      "          -1.1339e+00,  6.7062e-01, -9.3956e-01,  1.0433e+00,  1.2892e+00,\n",
      "           3.4359e-01,  1.1482e+00, -8.7999e-01,  1.2383e-01, -3.1854e-01,\n",
      "           4.8291e-01,  1.0494e+00, -1.9003e+00, -1.2830e+00, -9.5807e-01,\n",
      "          -1.2289e+00,  3.9979e-01, -3.1717e-01,  4.7813e-01,  7.6217e-02],\n",
      "         [ 2.0923e-01, -4.3880e-01,  8.9921e-01,  1.0472e+00,  8.9408e-01,\n",
      "          -2.5113e-02,  4.0156e-01, -1.4410e-01, -4.4864e-01, -1.0465e+00,\n",
      "          -1.4124e-03,  4.6393e-01,  4.8102e-01,  1.3084e+00,  9.3389e-02,\n",
      "           1.5007e-02, -1.4917e-01, -1.5022e-01, -1.0368e+00, -8.5161e-02,\n",
      "           7.9488e-01, -7.4991e-01, -1.0572e+00, -7.0851e-01, -1.9012e+00,\n",
      "          -1.2395e+00,  1.0293e+00, -1.9068e-01,  1.9986e-01, -1.4368e+00,\n",
      "           1.3727e+00, -6.2847e-01,  4.7868e-02,  1.6451e+00, -8.5051e-01,\n",
      "           8.4190e-01, -1.1600e+00, -1.3215e+00,  6.6054e-01, -8.6883e-01,\n",
      "          -3.5961e-01,  3.6654e-01, -8.5669e-01,  1.4699e+00, -6.4307e-01,\n",
      "           3.7110e-01,  2.7913e-01,  1.5388e+00, -1.0213e+00,  5.7951e-01,\n",
      "           3.7242e-01,  2.8739e-01,  1.2131e+00,  3.9396e-01,  2.7943e-01,\n",
      "           2.5147e+00,  1.1248e+00, -3.4092e-01, -1.0510e+00,  6.7265e-01,\n",
      "           4.9775e-01, -1.1061e-01,  6.8786e-01, -6.3160e-01, -5.3317e-02,\n",
      "          -9.3879e-01, -1.3132e+00, -1.0131e+00,  3.5663e-01, -2.8064e-01,\n",
      "           1.7025e+00, -1.8907e-01, -3.7804e-01,  6.6660e-01, -1.0407e+00,\n",
      "          -8.8604e-01,  6.2218e-01,  6.9535e-01,  2.4284e-01, -8.4982e-01,\n",
      "          -6.0377e-01,  9.8470e-01,  1.2302e+00,  3.9680e-01,  2.3001e-01,\n",
      "          -1.8540e+00, -2.2511e-01,  3.9071e-02,  3.7564e-01,  1.3712e+00,\n",
      "           5.3146e-01, -5.7102e-01,  1.1351e+00, -1.3441e-01,  8.9552e-01,\n",
      "          -3.5479e+00, -3.3248e-02, -8.1134e-01, -1.0841e+00, -2.1817e+00],\n",
      "         [ 2.9951e+00,  9.3334e-01, -1.9657e-01,  8.0513e-01,  2.7115e-01,\n",
      "           4.0293e-01,  4.2464e-01,  1.3873e-01, -1.5221e+00,  1.9006e+00,\n",
      "           5.7479e-02,  7.2052e-01, -3.9953e-01, -1.7248e+00,  2.0088e-01,\n",
      "          -6.3976e-01, -3.0485e+00,  1.4028e+00, -8.0569e-01, -1.0111e+00,\n",
      "           2.0506e-01,  2.5348e-02, -1.4447e+00,  3.4034e-01,  1.1669e+00,\n",
      "           1.1348e+00, -4.1431e-02, -1.2243e+00, -1.3219e+00,  3.7198e-01,\n",
      "           6.7208e-01, -4.1569e-01,  9.0853e-01, -1.9413e-01, -1.2330e+00,\n",
      "           1.5377e-01,  5.8484e-01,  4.8072e-02,  1.7658e+00, -5.2816e-01,\n",
      "           1.0660e-01,  1.7476e+00,  9.1789e-01, -1.6851e+00, -5.9671e-01,\n",
      "           8.3849e-01,  6.8043e-01,  2.7340e-01,  8.3205e-01, -4.8172e-01,\n",
      "          -1.8986e-01,  3.6274e-01, -4.7938e-01, -9.0388e-01, -9.6840e-01,\n",
      "           1.2206e+00, -2.0761e+00, -6.9212e-01,  7.8321e-01,  1.1572e+00,\n",
      "           1.0103e+00,  1.5243e+00, -1.7116e+00, -1.1846e+00,  1.8180e+00,\n",
      "           1.3030e+00, -4.5418e-01, -8.1444e-01,  4.8987e-01,  4.1548e-01,\n",
      "          -1.2686e+00, -7.6133e-01, -1.3503e-01,  1.9663e+00, -3.9708e-01,\n",
      "          -1.4214e+00,  1.3397e+00, -8.0893e-01, -1.3174e+00,  7.3130e-01,\n",
      "          -2.3922e-01,  8.0627e-01, -9.6207e-01,  2.8350e-01, -1.9198e-01,\n",
      "           7.4467e-01,  6.4731e-01, -1.2562e+00,  2.2980e-01, -9.7771e-01,\n",
      "           2.0896e-01,  8.0025e-02, -2.3990e+00,  5.7385e-01, -9.1853e-01,\n",
      "          -1.1649e+00, -1.4111e+00, -5.8991e-01, -5.7639e-01, -1.7941e+00],\n",
      "         [-2.0216e+00,  1.2630e+00,  3.5475e-03, -5.8727e-01,  1.1957e-01,\n",
      "          -7.0529e-03, -4.5383e-01, -9.1047e-01, -5.0323e-01, -1.5149e+00,\n",
      "          -1.6272e+00,  1.1651e+00,  1.8545e+00, -7.2740e-02, -9.0745e-01,\n",
      "          -1.1476e-01, -1.8505e+00, -1.0764e+00,  2.5471e-01,  2.9272e-02,\n",
      "          -2.1896e+00,  4.0437e-01, -4.0191e-01,  2.7202e-01, -3.2640e-01,\n",
      "          -4.5559e-01, -5.8604e-01, -8.4593e-01,  1.1042e+00,  2.1289e+00,\n",
      "           1.4482e-01,  8.9400e-01, -3.3082e-01, -1.1579e+00,  2.7613e-01,\n",
      "          -1.7615e+00,  1.6152e+00, -4.2328e-01, -1.1928e-01, -1.4287e+00,\n",
      "           4.3252e-01,  4.1699e-01, -7.0662e-01,  3.2328e-01,  9.2728e-01,\n",
      "           1.2707e-01,  7.2064e-01, -2.7514e+00,  8.7236e-01, -1.6260e+00,\n",
      "          -1.1574e+00,  7.5620e-01,  4.4498e-01, -5.7639e-01, -1.5207e+00,\n",
      "           3.7736e-01,  1.0113e+00,  1.8918e-01, -1.3529e+00, -1.9467e+00,\n",
      "           1.4520e+00, -8.6347e-01, -3.3625e-01, -1.2619e+00,  9.5755e-01,\n",
      "           1.1984e+00, -9.0905e-01, -4.8091e-01, -1.6833e-01,  1.3193e+00,\n",
      "          -5.7257e-02,  3.5011e-01,  1.5601e+00,  1.2147e+00,  1.0007e+00,\n",
      "           2.7043e-01,  2.0730e-01,  7.2363e-01,  1.3975e-02,  1.0154e+00,\n",
      "          -3.5378e-01, -1.6333e+00,  7.9186e-01,  1.5740e+00,  2.4323e-01,\n",
      "          -4.8113e-01,  5.6658e-01,  1.3501e+00,  1.0853e-01,  1.0851e+00,\n",
      "          -5.7983e-01,  3.1403e-01, -1.4934e+00, -6.3227e-01,  3.0361e-01,\n",
      "          -1.2283e+00, -1.1619e+00,  1.6045e+00,  9.0209e-01, -9.3277e-01]]],\n",
      "       grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# 1. Define the layer\n",
    "# 1000 words in vocab, each represented by a 100-dim vector\n",
    "embedding = nn.Embedding(1000, 100)\n",
    "\n",
    "# 2. Create some \"input_ids\" (e.g., a batch of 2 sentences, 4 tokens each)\n",
    "input_ids = torch.tensor([[1, 42, 5, 9], [10, 3, 999, 0]])\n",
    "\n",
    "# 3. Pass through the embedding layer\n",
    "embedded_output = embedding(input_ids)\n",
    "\n",
    "print(f\"Input Shape:  {input_ids}\")      # torch.Size([2, 4])\n",
    "print(f\"Output Shape: {embedded_output}\") # torch.Size([2, 4, 100])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
