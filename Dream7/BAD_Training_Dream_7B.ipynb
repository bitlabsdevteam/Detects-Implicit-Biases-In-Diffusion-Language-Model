{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45938750",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title 1. Setup & Installation\n",
    "\n",
    "print(\" Installing optimized stack \\n\")\n",
    "# We use sdpa (built-in), so no need for flash-attn pip install\n",
    "!pip install -q -U torch transformers==4.37.0 bitsandbytes accelerate  datasets huggingface_hub tqdm scikit-learn matplotlib seaborn pandas safetensors\n",
    "\n",
    "print(\" Installation complete!\\n\")\n",
    "print(\"âœ… Installation complete!\\n\")\n",
    "\n",
    "# Verify installation\n",
    "print(\"ğŸ“¦ Verifying package versions:\")\n",
    "!pip show torch transformers bitsandbytes accelerate | grep \"Name:\\|Version:\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdda5843",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Load Environment Variables\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from huggingface_hub import login\n",
    "\n",
    "load_dotenv()\n",
    "login(token=os.getenv(\"HUGGING_FACE_HUB_TOKEN\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63e2292b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Import Libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import random\n",
    "from tqdm.auto import tqdm\n",
    "from datetime import datetime\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "import os\n",
    "import gc\n",
    "import platform\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM\n",
    "    # BitsAndBytesConfig removed: Not compatible with MPS\n",
    ")\n",
    "from datasets import load_dataset\n",
    "from huggingface_hub import HfApi, create_repo, login\n",
    "from safetensors.torch import save_file\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GroupShuffleSplit\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# 1. Global Determinism Anchor (MPS Compatible)\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "os.environ['PYTHONHASHSEED'] = str(SEED)\n",
    "\n",
    "if torch.backends.mps.is_available():\n",
    "    # MPS does not currently support a global manual_seed_all like CUDA\n",
    "    # but setting the manual_seed covers the generator.\n",
    "    torch.mps.manual_seed(SEED)\n",
    "\n",
    "# 2. Device Detection Logic\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "    precision_mode = \"Float16 (MPS Optimized)\"\n",
    "    compute_dtype = torch.float16\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    precision_mode = \"Float16 (CUDA)\"\n",
    "    compute_dtype = torch.float16\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    precision_mode = \"Float32 (CPU Fallback)\"\n",
    "    compute_dtype = torch.float32\n",
    "\n",
    "# 3. Environment Forensic Dashboard\n",
    "print(\"=\"*80)\n",
    "print(\" FAIRSTEER RESEARCH SUITE: APPLE SILICON EDITION\")\n",
    "print(\"=\"*80)\n",
    "print(f\" System OS:       {platform.system()} {platform.release()}\")\n",
    "print(f\" Processor:       {platform.processor()}\")\n",
    "print(f\" Active Device:   {device.type.upper()}\")\n",
    "print(f\" Logic Precision: {precision_mode}\")\n",
    "\n",
    "if device.type == \"mps\":\n",
    "    # Note: MPS does not provide direct VRAM 'total' queries through torch yet.\n",
    "    # It shares the system's Unified Memory.\n",
    "    print(\" Architecture:    Unified Memory Architecture (UMA)\")\n",
    "    print(\" Performance:     Metal Performance Shaders (MPS) Active\")\n",
    "\n",
    "print(\"=\"*80 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d42c385",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title 3. Configuration (Dream-7B Diffusion Edition)\n",
    "\n",
    "import torch\n",
    "from typing import List, Optional\n",
    "\n",
    "class TrainingConfig:\n",
    "    # --- MODEL & ARCHITECTURE (Dream-7B/LLaDA Specific) ---\n",
    "    # Dream-v0 is a bidirectional Transformer diffusion model\n",
    "    base_model_name = \"Dream-org/Dream-v0-Instruct-7B\"\n",
    "    model_hidden_dim = 4096\n",
    "    max_seq_len = 1024 # Standard context window for extraction\n",
    "    num_layers = 32\n",
    "    \n",
    "    # --- DIFFUSION PARAMETERS ---\n",
    "    # Extraction at t=0.5 (Maximum semantic ambiguity, ideal for BAD probe training)\n",
    "    extraction_t = 0.5 \n",
    "    # Dream-7B Tokenizer Specifics\n",
    "    # We will fetch this dynamically in the loader, but standard is 126348 or similar\n",
    "    mask_token_id = None \n",
    "    \n",
    "    # --- DATASET CONFIGURATION (FairSteer Standard) ---\n",
    "    bbq_dataset_name = \"bitlabsdb/BBQ_dataset\"\n",
    "    bbq_target_loc_dataset = \"bitlabsdb/bbq_target_loc_dedup\"\n",
    "    MMLU_DATASET = \"bitlabsdb/MMLU\"\n",
    "    BBQA_DATASET = \"bitlabsdb/BBQA\"\n",
    "    \n",
    "    # Manifold Sizes (Scaled for 7B VRAM limits)\n",
    "    num_bbq_samples = 20 \n",
    "    mmlu_data_size = 4   \n",
    "    DSV_TARGET = 110       \n",
    "    \n",
    "    # --- HARDWARE & PIPELINE OPTIMIZATION ---\n",
    "    train_val_split = 0.8\n",
    "    batch_size = 32\n",
    "    # LLaDA/Dream forward passes are memory-heavy due to bidirectional attention\n",
    "    # We reduce batch size to ensure FlashAttention/SDPA stability\n",
    "    extraction_batch_size = 4 \n",
    "    SEED = 42\n",
    "    \n",
    "    # Full Layer Scanning for the \"Diffusion Causal Bottleneck\"\n",
    "    candidate_layers_range = list(range(0, 32))\n",
    "    \n",
    "    # --- LABELS (FairSteer Aligned) ---\n",
    "    LABEL_BIASED = 0\n",
    "    LABEL_UNBIASED = 1\n",
    "\n",
    "    # --- SAVE & DEPLOYMENT ---\n",
    "    hf_repo_name = \"bitlabsdb/Dream-7B-FairSteer-BAD\"\n",
    "    local_save_dir = \"./artifacts_dream_7b\"\n",
    "    # Dream-v0 Instruct uses specific prompt templates (Algorithm 2)\n",
    "    with_header = True \n",
    "    \n",
    "    IS_DEBUG = False\n",
    "\n",
    "    @property\n",
    "    def model_id_short(self) -> str:\n",
    "        return self.base_model_name.split(\"/\")[-1]\n",
    "\n",
    "config = TrainingConfig()\n",
    "\n",
    "# Global Environment Forensic Dashboard\n",
    "print(\"=\"*80)\n",
    "print(\" ğŸ›¡ï¸ CONFIGURATION - DREAM-7B DIFFUSION RESEARCH SUITE\")\n",
    "print(\"=\"*80)\n",
    "print(f\"   â€¢ Model Path:         {config.base_model_name}\")\n",
    "print(f\"   â€¢ Architecture:       Masked Diffusion (Bidirectional)\")\n",
    "print(f\"   â€¢ Logic Precision:    {precision_mode}\")\n",
    "print(f\"   â€¢ Extraction Strategy: Token-wise Masked Pooling at t={config.extraction_t}\")\n",
    "print(f\"   â€¢ GPU Optimization:   SDPA / FlashAttention Active\")\n",
    "print(\"=\"*80 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d045bc2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title 5. BBQ Dataset Integrated Merging (Composite Key Integrity) - Merged by example_id & category\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "import warnings\n",
    "\n",
    "def load_and_merge_bbq(config) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Loads BBQ and Targets, merges via Composite Key, and performs config-driven\n",
    "    sub-sampling to output the final training manifold.\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"=\"*80)\n",
    "    print(\" ğŸš€ RESEARCH-GRADE BBQ LOADER & SUB-SAMPLER (COMPOSITE KEY VERSION)\")\n",
    "    print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "    # 1. DATA ACQUISITION\n",
    "    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "    print(\"1. Loading Primary BBQ Dataset...\")\n",
    "    try:\n",
    "        # Use config-defined path if available\n",
    "        ds_name = getattr(config, 'bbq_dataset_name', \"bitlabsdb/BBQ_dataset\")\n",
    "        bbq_ds = load_dataset(ds_name, split=\"train\")\n",
    "    except Exception as e:\n",
    "        print(f\"   âš ï¸ Primary loading failed: {e}. Attempting fallback...\")\n",
    "        bbq_ds = load_dataset(\"bitlabsdb/BBQ_dataset\", split=\"train\")\n",
    "\n",
    "    df_bbq = pd.DataFrame(bbq_ds)\n",
    "    df_bbq['example_id'] = pd.to_numeric(df_bbq['example_id'], errors='coerce').fillna(-1).astype(int)\n",
    "    print(f\"   âœ… Primary BBQ Loaded: {len(df_bbq):,} rows.\")\n",
    "\n",
    "    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "    # 2. METADATA PREPARATION (Stereotype Targets)\n",
    "    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "    print(\"\\n2. Loading Target Locations (Stereotype Metadata)...\")\n",
    "    try:\n",
    "        loc_ds = load_dataset(\"bitlabsdb/bbq_target_loc_dedup\", split=\"train\")\n",
    "    except:\n",
    "        # Fallback to local script if needed\n",
    "        loc_ds = load_dataset(\"bitlabsdb/bbq_target_loc_dedup\", split=\"train\")\n",
    "\n",
    "    df_loc = pd.DataFrame(loc_ds)\n",
    "    df_loc['example_id'] = pd.to_numeric(df_loc['example_id'], errors='coerce').dropna().astype(int)\n",
    "    df_loc['target_loc'] = pd.to_numeric(df_loc['target_loc'], errors='coerce')\n",
    "    df_loc = df_loc[df_loc['target_loc'].isin([0, 1, 2])]\n",
    "    df_loc['target_loc'] = df_loc['target_loc'].astype(int)\n",
    "\n",
    "    # Deduplicate on Composite Key (ID + Category) to ensure 1:1 mapping\n",
    "    df_loc = df_loc.drop_duplicates(subset=['example_id', 'category'], keep='first')\n",
    "    print(f\"   âœ… Target Metadata Prepared: {len(df_loc):,} unique causal pairs.\")\n",
    "\n",
    "    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "    # 3. COMPOSITE MERGE & INTEGRITY AUDIT\n",
    "    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "    print(\"\\n3. Executing Composite Merge & Integrity Audit...\")\n",
    "    integrity_check = pd.merge(\n",
    "        df_bbq,\n",
    "        df_loc[['example_id', 'category', 'target_loc']],\n",
    "        on=['example_id', 'category'],\n",
    "        how='left',\n",
    "        indicator=True\n",
    "    )\n",
    "\n",
    "    df_merged = integrity_check[integrity_check['_merge'] == 'both'].drop(columns=['_merge']).copy()\n",
    "    df_missing = integrity_check[integrity_check['_merge'] == 'left_only'].copy()\n",
    "\n",
    "    count_total = len(df_bbq)\n",
    "    count_merged = len(df_merged)\n",
    "\n",
    "    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "    # 4. CONFIG-DRIVEN SUB-SAMPLING (THE \"FAIRSTEER\" FILTER)\n",
    "    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "    if hasattr(config, 'num_bbq_samples') and config.num_bbq_samples is not None:\n",
    "        if config.num_bbq_samples < count_merged:\n",
    "            print(f\"\\nâœ‚ï¸  APPLYING SUB-SAMPLING: Filtering to {config.num_bbq_samples:,} samples...\")\n",
    "            # We sample deterministically using config.SEED to maintain research reproducibility\n",
    "            df_final = df_merged.sample(n=config.num_bbq_samples, random_state=config.SEED).copy()\n",
    "        else:\n",
    "            print(f\"\\nâ„¹ï¸  Config limit ({config.num_bbq_samples:,}) exceeds available merged data. Using all merged records.\")\n",
    "            df_final = df_merged.copy()\n",
    "    else:\n",
    "        print(\"\\nâ„¹ï¸  No sub-sampling limit found in config. Using full merged dataset.\")\n",
    "        df_final = df_merged.copy()\n",
    "\n",
    "    count_final = len(df_final)\n",
    "\n",
    "    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "    # 5. RESEARCH DASHBOARD\n",
    "    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "    print(\"\\nğŸ“Š Generating Research Dashboard...\")\n",
    "    sns.set_theme(style=\"whitegrid\")\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(22, 7))\n",
    "    plt.suptitle(f\"BBQ Data Curation Pipeline (Final N={count_final:,})\", fontsize=18, weight='bold', y=1.02)\n",
    "\n",
    "    # Plot 1: Attrition Flow\n",
    "    retention_labels = ['Total Input', 'Valid Merged', 'Config Final']\n",
    "    retention_values = [count_total, count_merged, count_final]\n",
    "    sns.barplot(x=retention_labels, y=retention_values, palette='Blues_r', ax=axes[0], edgecolor='black')\n",
    "    axes[0].set_title(\"Data Retention Flow\", fontsize=14, weight='bold')\n",
    "    for i, v in enumerate(retention_values):\n",
    "        axes[0].text(i, v + (count_total * 0.02), f\"{v:,}\", ha='center', weight='bold')\n",
    "\n",
    "    # Plot 2: Final Categorical Distribution\n",
    "    sns.countplot(\n",
    "        y='category',\n",
    "        data=df_final,\n",
    "        order=df_final['category'].value_counts().index,\n",
    "        palette=\"viridis\",\n",
    "        ax=axes[1],\n",
    "        edgecolor='black'\n",
    "    )\n",
    "    axes[1].set_title(f\"Final Category Mix (N={count_final:,})\", fontsize=14, weight='bold')\n",
    "\n",
    "    # Plot 3: Context Distribution (Ambig vs Disambig)\n",
    "    df_final['context_condition'].value_counts().plot.pie(\n",
    "        autopct='%1.1f%%', colors=['#74b9ff', '#fab1a0'], ax=axes[2],\n",
    "        startangle=140, wedgeprops={'edgecolor':'black'}\n",
    "    )\n",
    "    axes[2].set_ylabel('')\n",
    "    axes[2].set_title(\"Context Composition\", fontsize=14, weight='bold')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(f\"ğŸ’ FINAL MANIFOLD READY: {count_final:,} records assigned to 'bbq_merged_df'\")\n",
    "    print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "    return df_final\n",
    "\n",
    "# Execute the pipeline and populate the global variable\n",
    "bbq_merged_df = load_and_merge_bbq(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5482fbb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title 5.2 BBQA Dataset Integrated Merging (Composite Key Integrity)\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "import warnings\n",
    "\n",
    "def load_and_merge_bbqa(config) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    OpenAI/MIT Standard: Manifold Curation Engine.\n",
    "\n",
    "    Loads the refined BBQA dataset and merges with stereotype target metadata\n",
    "    using composite key integrity (example_id + category).\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"=\"*80)\n",
    "    print(\" ğŸš€ RESEARCH-GRADE BBQ LOADER: REFINED MANIFOLD EDITION\")\n",
    "    print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "    # 1. DATA ACQUISITION: Refined BBQA\n",
    "    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "    print(\"1. Loading Primary BBQA Dataset [bitlabsdb/BBQA]...\")\n",
    "    try:\n",
    "        # Targeting the specific refined repo as requested\n",
    "        bbq_ds = load_dataset(\"bitlabsdb/BBQA\", split=\"train\")\n",
    "    except Exception as e:\n",
    "        print(f\"   âŒ CRITICAL: Primary loading failed: {e}\")\n",
    "        return None\n",
    "\n",
    "    df_bbq = pd.DataFrame(bbq_ds)\n",
    "\n",
    "    # Forensic Type Casting: Ensure example_id is a strictly typed integer\n",
    "    df_bbq['example_id'] = pd.to_numeric(df_bbq['example_id'], errors='coerce').fillna(-1).astype(int)\n",
    "    print(f\"   âœ… Primary BBQA Loaded: {len(df_bbq):,} rows.\")\n",
    "\n",
    "    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "    # 2. METADATA PREPARATION: Stereotype Targets\n",
    "    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "    print(\"\\n2. Loading Target Locations [bitlabsdb/bbq_target_loc_dedup]...\")\n",
    "    try:\n",
    "        loc_ds = load_dataset(\"bitlabsdb/bbq_target_loc_dedup\", split=\"train\")\n",
    "    except Exception as e:\n",
    "        print(f\"   âŒ CRITICAL: Target metadata loading failed: {e}\")\n",
    "        return None\n",
    "\n",
    "    df_loc = pd.DataFrame(loc_ds)\n",
    "\n",
    "    # Normalizing metadata keys\n",
    "    df_loc['example_id'] = pd.to_numeric(df_loc['example_id'], errors='coerce').dropna().astype(int)\n",
    "    df_loc['target_loc'] = pd.to_numeric(df_loc['target_loc'], errors='coerce')\n",
    "\n",
    "    # Causal Guard: Filter for valid choice indices (A=0, B=1, C=2)\n",
    "    df_loc = df_loc[df_loc['target_loc'].isin([0, 1, 2])]\n",
    "    df_loc['target_loc'] = df_loc['target_loc'].astype(int)\n",
    "\n",
    "    # Deduplicate on Composite Key to ensure 1:1 causal mapping\n",
    "    df_loc = df_loc.drop_duplicates(subset=['example_id', 'category'], keep='first')\n",
    "    print(f\"   âœ… Target Metadata Prepared: {len(df_loc):,} unique causal pairs.\")\n",
    "\n",
    "    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "    # 3. COMPOSITE MERGE: Semantic Alignment\n",
    "    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "    print(\"\\n3. Executing Composite Merge & Integrity Audit...\")\n",
    "\n",
    "    # We use a LEFT join on both ID and Category to preserve semantic context\n",
    "    integrity_check = pd.merge(\n",
    "        df_bbq,\n",
    "        df_loc[['example_id', 'category', 'target_loc']],\n",
    "        on=['example_id', 'category'],\n",
    "        how='left',\n",
    "        indicator=True\n",
    "    )\n",
    "\n",
    "    # Identify successful manifold alignments\n",
    "    df_merged = integrity_check[integrity_check['_merge'] == 'both'].drop(columns=['_merge']).copy()\n",
    "\n",
    "    count_total = len(df_bbq)\n",
    "    count_merged = len(df_merged)\n",
    "\n",
    "    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "    # 4. CONFIG-DRIVEN SUB-SAMPLING\n",
    "    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "    if hasattr(config, 'num_bbq_samples') and config.num_bbq_samples is not None:\n",
    "        if config.num_bbq_samples < count_merged:\n",
    "            print(f\"\\nâœ‚ï¸  APPLYING SUB-SAMPLING: Filtering to {config.num_bbq_samples:,} samples...\")\n",
    "            df_final = df_merged.sample(n=config.num_bbq_samples, random_state=config.SEED).copy()\n",
    "        else:\n",
    "            print(f\"\\nâ„¹ï¸  Config limit exceeds available data. Using all {count_merged:,} records.\")\n",
    "            df_final = df_merged.copy()\n",
    "    else:\n",
    "        df_final = df_merged.copy()\n",
    "\n",
    "    count_final = len(df_final)\n",
    "\n",
    "    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "    # 5. RESEARCH DASHBOARD: Manifold Distribution\n",
    "    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "    print(\"\\nğŸ“Š Visualizing Final Training Manifold...\")\n",
    "    sns.set_theme(style=\"whitegrid\")\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(22, 7))\n",
    "    plt.suptitle(f\"BBQA Data Curation Pipeline (N={count_final:,})\", fontsize=18, weight='bold', y=1.02)\n",
    "\n",
    "    # Attrition Flow\n",
    "    retention_labels = ['Input (BBQA)', 'Valid Merged', 'Sub-sampled']\n",
    "    retention_values = [count_total, count_merged, count_final]\n",
    "    sns.barplot(x=retention_labels, y=retention_values, palette='Blues_r', ax=axes[0], edgecolor='black')\n",
    "    axes[0].set_title(\"Data Retention Flow\", fontsize=14, weight='bold')\n",
    "\n",
    "    # Category Mix\n",
    "    sns.countplot(\n",
    "        y='category',\n",
    "        data=df_final,\n",
    "        order=df_final['category'].value_counts().index,\n",
    "        palette=\"viridis\",\n",
    "        ax=axes[1],\n",
    "        edgecolor='black'\n",
    "    )\n",
    "    axes[1].set_title(\"Categorical Saturation\", fontsize=14, weight='bold')\n",
    "\n",
    "    # Ambig vs Disambig\n",
    "    df_final['context_condition'].value_counts().plot.pie(\n",
    "        autopct='%1.1f%%', colors=['#74b9ff', '#fab1a0'], ax=axes[2],\n",
    "        startangle=140, wedgeprops={'edgecolor':'black'}\n",
    "    )\n",
    "    axes[2].set_title(\"Context Composition\", fontsize=14, weight='bold')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(f\"ğŸ’ FINAL MANIFOLD READY: {count_final:,} records assigned to 'bbq_merged_df'\")\n",
    "    print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "    return df_final\n",
    "\n",
    "# Execute the pipeline\n",
    "bbqa_merged_df = load_and_merge_bbqa(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adbc9b5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title 7.5 MMLU Anchor Analytics: Knowledge Manifold Breadth - Configurable Anchor\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datasets import load_dataset\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\" ğŸ”¬ ANALYZING MMLU KNOWLEDGE ANCHOR DISTRIBUTION\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "# 1. Load MMLU Metadata\n",
    "print(f\"ğŸ“¥ Fetching MMLU subjects from {config.MMLU_DATASET}...\")\n",
    "mmlu_ds = load_dataset(config.MMLU_DATASET, split=\"train\")\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# NEW FEATURE: CONFIGURABLE MMLU ANCHOR SIZE (mmlu_anchor_size)\n",
    "# ---------------------------------------------------------\n",
    "if hasattr(config, 'mmlu_data_size') and config.mmlu_data_size is not None:\n",
    "    total_mmlu = len(mmlu_ds)\n",
    "    if config.mmlu_data_size < total_mmlu:\n",
    "        print(f\"âœ‚ï¸ Applying MMLU Anchor Sampling: {config.mmlu_data_size} records...\")\n",
    "        # Deterministic sampling\n",
    "        mmlu_ds = mmlu_ds.shuffle(seed=config.SEED).select(range(config.mmlu_data_size))\n",
    "    else:\n",
    "        print(f\"â„¹ï¸ Config 'mmlu_data_size' ({config.mmlu_data_size}) >= Available. Using full MMLU set.\")\n",
    "\n",
    "df_mmlu = pd.DataFrame(mmlu_ds)\n",
    "\n",
    "# 2. Extract Top Subjects for Visualization\n",
    "subject_counts = df_mmlu['subject'].value_counts().head(20)\n",
    "\n",
    "# 3. Generating Visualization (Matching BBQ Style)\n",
    "sns.set_theme(style=\"whitegrid\", context=\"talk\")\n",
    "plt.figure(figsize=(14, 8))\n",
    "\n",
    "ax = sns.barplot(\n",
    "    x=subject_counts.values,\n",
    "    y=subject_counts.index,\n",
    "    palette=\"magma\",\n",
    "    edgecolor='black',\n",
    "    linewidth=1.2\n",
    ")\n",
    "\n",
    "plt.title(f\"MMLU Anchor Distribution: Top 20 Knowledge Domains\\n(Total Active Anchor N = {len(df_mmlu):,})\",\n",
    "          fontsize=16, weight='bold', pad=20)\n",
    "plt.xlabel(\"Count of Factual Samples (Labeled Strictly 1/Unbiased)\", fontsize=13, weight='bold')\n",
    "plt.ylabel(\"MMLU Subject Area\", fontsize=13, weight='bold')\n",
    "\n",
    "for p in ax.patches:\n",
    "    width = p.get_width()\n",
    "    ax.annotate(f'{int(width)}',\n",
    "                (width, p.get_y() + p.get_height() / 2),\n",
    "                xytext=(5, 0), textcoords='offset points',\n",
    "                ha='left', va='center', fontsize=11, weight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"âœ… Knowledge Anchor Verified.\")\n",
    "print(f\"   â€¢ Total MMLU Samples (Used): {len(df_mmlu):,}\")\n",
    "print(f\"   â€¢ Unique Domains:            {df_mmlu['subject'].nunique()}\")\n",
    "print(\"   â€¢ Research Impact:           Ensures the BAD probe distinguishes 'Bias' from 'General Knowledge'.\")\n",
    "print(\"=\"*80 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c937a73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title 7.6 Joint Manifold Balance: BBQ vs. MMLU Anchor\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# 1. DYNAMIC DATA COUNTING (Aligning with config.num_bbq_samples)\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# We use the current state of bbq_merged_df which was sub-sampled in Cell 6\n",
    "# to match your config.num_bbq_samples.\n",
    "bbq_count = len(bbq_merged_df)\n",
    "mmlu_count = len(df_mmlu)\n",
    "\n",
    "# Validation check to ensure config compliance\n",
    "if hasattr(config, 'num_bbq_samples') and config.num_bbq_samples is not None:\n",
    "    # If the user requested fewer than available, the count should match config\n",
    "    target_bbq = config.num_bbq_samples\n",
    "    print(f\"â„¹ï¸  Configured BBQ Target: {target_bbq:,} samples.\")\n",
    "else:\n",
    "    print(f\"â„¹ï¸  No sub-sampling requested. Using full BBQ manifold.\")\n",
    "\n",
    "labels = ['BBQ Bias Distribution', 'MMLU Knowledge Anchor']\n",
    "sizes = [bbq_count, mmlu_count]\n",
    "total_n = sum(sizes)\n",
    "colors = ['#ff7675', '#0984e3'] # FairSteer Brand Colors: Red (Bias) vs Blue (Knowledge)\n",
    "explode = (0, 0.1)  # Surgically separate the MMLU Anchor for visual emphasis\n",
    "\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# 2. GENERATE PROFESSIONAL MANIFOLD VISUALIZATION\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "sns.set_theme(style=\"white\")\n",
    "fig, ax = plt.subplots(figsize=(10, 8), dpi=150)\n",
    "\n",
    "patches, texts, autotexts = ax.pie(\n",
    "    sizes,\n",
    "    explode=explode,\n",
    "    labels=labels,\n",
    "    colors=colors,\n",
    "    autopct=lambda p: f'{p:.1f}%\\n({int(p * total_n / 100):,} samples)',\n",
    "    shadow=True,\n",
    "    startangle=140,\n",
    "    textprops={'fontsize': 12, 'weight': 'bold'},\n",
    "    pctdistance=0.75\n",
    ")\n",
    "\n",
    "# Maintain visual clarity for white/dark backgrounds\n",
    "for autotext in autotexts:\n",
    "    autotext.set_color('white')\n",
    "\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# 3. RESEARCH CONTEXT & SUMMARY\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "plt.title(f\"FairSteer Training Manifold: {config.base_model_name.split('/')[-1]}\\n\"\n",
    "          f\"(Unified Dataset N = {total_n:,})\",\n",
    "          fontsize=16, weight='bold', pad=20)\n",
    "\n",
    "# Create a clean, scientific legend\n",
    "plt.legend(\n",
    "    patches,\n",
    "    [f\"{l}: {s:,}\" for l, s in zip(labels, sizes)],\n",
    "    title=\"Latent Sources\",\n",
    "    loc=\"center left\",\n",
    "    bbox_to_anchor=(1, 0, 0.5, 1),\n",
    "    frameon=True,\n",
    "    shadow=True\n",
    ")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ğŸ“ ARCHITECT'S MANIFOLD SUMMARY (CONFIG COMPLIANT)\")\n",
    "print(\"=\"*80)\n",
    "print(f\"â€¢ Config Load Limit:    {config.num_bbq_samples if config.num_bbq_samples else 'None':<10}\")\n",
    "print(f\"â€¢ Active BBQ Samples:   {bbq_count:,}\")\n",
    "print(f\"â€¢ Active MMLU Anchor:   {mmlu_count:,}\")\n",
    "print(f\"â€¢ Combined Saturation:  {total_n:,} snapshots.\")\n",
    "print(f\"â€¢ Anchor Strength:      {(mmlu_count/total_n):.1%} of the total training manifold.\")\n",
    "print(\"-\" * 80)\n",
    "print(\"â€¢ Causal Prediction:    The resulting BAD probe will distinguish between\")\n",
    "print(\"                         'Logical Certainty' (MMLU) and 'Social Bias' (BBQ).\")\n",
    "print(\"=\"*80 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a971b8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title 8. Production-Grade Hook Manager (LLaDA Bidirectional Sniper Edition)\n",
    "import torch\n",
    "import numpy as np\n",
    "from typing import List, Dict, Optional\n",
    "import weakref\n",
    "\n",
    "class MultiLayerHookManager:\n",
    "    \"\"\"\n",
    "    LLaDA RESEARCH STANDARD: Masked-Mean Sniper.\n",
    "    Surgically extracts activations ONLY from [MASK] token positions.\n",
    "    Collapses bidirectional reasoning into a singular manifold via mean-pooling.\n",
    "    \"\"\"\n",
    "    _active_managers = weakref.WeakSet()\n",
    "\n",
    "    def __init__(self, model, layer_indices: List[int], mask_token_id: int):\n",
    "        self.model = model\n",
    "        self.layer_indices = sorted(layer_indices)\n",
    "        self.mask_token_id = mask_token_id\n",
    "        # Storage initialized on CPU for VRAM safety\n",
    "        self.activations: Dict[int, torch.Tensor] = {l: None for l in layer_indices}\n",
    "        self.hooks = []\n",
    "        self._is_registered = False\n",
    "        MultiLayerHookManager._active_managers.add(self)\n",
    "\n",
    "    def _create_hook_fn(self, layer_idx: int):\n",
    "        def hook_fn(module, input_args, output):\n",
    "            \"\"\"\n",
    "            input_args[0] is typically the input_ids (batch, seq_len)\n",
    "            output[0] is the residual stream (batch, seq_len, hidden_dim)\n",
    "            \"\"\"\n",
    "            # 1. Identify Mask coordinates from the input\n",
    "            # In LLaDA/Dream, input_args[0] contains the token IDs fed to the forward pass\n",
    "            input_ids = input_args[0] \n",
    "            mask_binary = (input_ids == self.mask_token_id) # Shape: (batch, seq_len)\n",
    "\n",
    "            # 2. Access residual stream\n",
    "            h = output[0] if isinstance(output, tuple) else output # (batch, seq_len, hidden)\n",
    "\n",
    "            # 3. LLaDA EXTRACTION LOGIC: Masked-Mean Pooling\n",
    "            # We only care about the vectors where the model is predicting tokens.\n",
    "            # We zero out non-masked positions and calculate the mean.\n",
    "            masked_h = h * mask_binary.unsqueeze(-1).to(h.dtype)\n",
    "            \n",
    "            # Sum across sequence length and divide by number of masks per sequence\n",
    "            sum_h = masked_h.sum(dim=1)\n",
    "            count_masks = mask_binary.sum(dim=1, keepdim=True).clamp(min=1)\n",
    "            mean_masked_h = sum_h / count_masks.to(h.dtype)\n",
    "\n",
    "            # 4. SNIPER OFFLOAD: Move to CPU immediately to keep GPU 0.0% fragmented\n",
    "            self.activations[layer_idx] = mean_masked_h.detach().clone().to('cpu', dtype=torch.float16)\n",
    "            \n",
    "        return hook_fn\n",
    "\n",
    "    def get_batch_manifold(self) -> torch.Tensor:\n",
    "        \"\"\"Reconstructs the multi-layer manifold on the CPU.\"\"\"\n",
    "        return torch.stack([self.activations[l] for l in self.layer_indices], dim=1)\n",
    "\n",
    "    def register(self):\n",
    "        if self._is_registered: return\n",
    "        for l in self.layer_indices:\n",
    "            # Dream-7B / LLaDA architecture path\n",
    "            target = self.model.model.layers[l]\n",
    "            self.hooks.append(target.register_forward_hook(self._create_hook_fn(l)))\n",
    "        self._is_registered = True\n",
    "\n",
    "    def clear(self):\n",
    "        \"\"\"Wipe CPU references to prepare for next batch.\"\"\"\n",
    "        for l in self.layer_indices:\n",
    "            self.activations[l] = None\n",
    "\n",
    "    def remove(self):\n",
    "        for h in self.hooks: h.remove()\n",
    "        self.hooks = []; self._is_registered = False\n",
    "\n",
    "    def __enter__(self): self.register(); return self\n",
    "    def __exit__(self, *args): self.remove(); torch.cuda.empty_cache()\n",
    "\n",
    "# MLP-Specific Sniper (Pointing to the FFN output instead of the total block output)\n",
    "class MLPHookManager(MultiLayerHookManager):\n",
    "    def register(self):\n",
    "        if self._is_registered: return\n",
    "        for l in self.layer_indices:\n",
    "            # Dream-7B uses .mlp naming convention\n",
    "            target = self.model.model.layers[l].mlp\n",
    "            self.hooks.append(target.register_forward_hook(self._create_hook_fn(l)))\n",
    "        self._is_registered = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbdf97c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title 9. Data Pipeline: Strict 1:1 Manifold Balancing - 1 Biased and 1 Unbiased mapping with GroupShuffleSplit\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "\n",
    "def prepare_data_pipeline(labels, config):\n",
    "    \"\"\"\n",
    "    Bytedance Standard: Strict 1 to 1 Undersampling.\n",
    "    Ensures the probe learns the bias manifold, not class frequency.\n",
    "\n",
    "    This revised version implements Group Based Stratification to prevent\n",
    "    contextual identity leakage between training and validation sets.\n",
    "    \"\"\"\n",
    "\n",
    "    # One. Identify indices for each pole\n",
    "    idx_biased = np.where(labels == 0)[0]\n",
    "    idx_neutral = np.where(labels == 1)[0]\n",
    "\n",
    "    # Two. Find the minority count for the limiting factor\n",
    "    n_samples_per_class = min(len(idx_biased), len(idx_neutral))\n",
    "\n",
    "    # Sanity Gate: Ensure class presence\n",
    "    if n_samples_per_class == 0:\n",
    "        raise ValueError(f\"Critical Error: One class has zero samples. \"\n",
    "                         f\"Biased: {len(idx_biased)}, Neutral: {len(idx_neutral)}.\")\n",
    "\n",
    "    # Three. Deterministic Downsampling for parity\n",
    "    # We use the seed for research reproducibility\n",
    "    rng = np.random.default_rng(config.SEED)\n",
    "    sampled_idx_biased = rng.choice(idx_biased, n_samples_per_class, replace=False)\n",
    "    sampled_idx_neutral = rng.choice(idx_neutral, n_samples_per_class, replace=False)\n",
    "\n",
    "    # Recombine balanced indices\n",
    "    balanced_indices = np.concatenate([sampled_idx_biased, sampled_idx_neutral])\n",
    "\n",
    "    # Four. Group Based Split Implementation\n",
    "    # Requirement: group_ids must be accessible via config or global namespace\n",
    "    # These represent the example_id or context identifiers from the BBQ dataset\n",
    "    if not hasattr(config, \"group_ids\"):\n",
    "        print(\"Warning: group_ids not found in config. Falling back to random split.\")\n",
    "        np.random.shuffle(balanced_indices)\n",
    "        split_point = int(len(balanced_indices) * 0.8)\n",
    "        train_set_idxs = balanced_indices[:split_point]\n",
    "        val_set_idxs = balanced_indices[split_point:]\n",
    "    else:\n",
    "        # Extract groups for the balanced subset\n",
    "        active_groups = config.group_ids[balanced_indices]\n",
    "        active_labels = labels[balanced_indices]\n",
    "\n",
    "        # Initialize the Group Based Stratification engine\n",
    "        gss = GroupShuffleSplit(n_splits=1, train_size=0.8, random_state=config.SEED)\n",
    "\n",
    "        # Perform the surgical split\n",
    "        # This ensures context isolation between training and validation manifolds\n",
    "        train_indices_rel, val_indices_rel = next(gss.split(balanced_indices, active_labels, active_groups))\n",
    "\n",
    "        # Map back to original manifold coordinates\n",
    "        train_set_idxs = balanced_indices[train_indices_rel]\n",
    "        val_set_idxs = balanced_indices[val_indices_rel]\n",
    "\n",
    "    # Five. Audit Logging\n",
    "    print(f\"FairSteer Manifold Audit (Grouped and Balanced):\")\n",
    "    print(f\"   Biased Pole at zero: {n_samples_per_class} samples\")\n",
    "    print(f\"   Neutral Pole at one: {n_samples_per_class} samples\")\n",
    "    print(f\"   Ratio: 1 to 1\")\n",
    "    print(f\"   Total Aligned Snapshots: {len(balanced_indices)}\")\n",
    "    print(f\"   Split: {len(train_set_idxs)} Train | {len(val_set_idxs)} Val\")\n",
    "    print(f\"   Status: Context leakage mitigated via Grouped Stratification\")\n",
    "\n",
    "    return train_set_idxs, val_set_idxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8ca53d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title 10. BAD Solver Library: Training Engine : sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, balanced_accuracy_score\n",
    "import torch\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "def train_probes(seed, train_set_idxs, val_set_idxs, activations, labels, num_layers):\n",
    "    \"\"\"\n",
    "    Forensic Training Engine for Biased Activation Detection.\n",
    "    Aligned with MIT standards for reproducibility and numerical stability.\n",
    "    \"\"\"\n",
    "    all_accs = []\n",
    "    all_bal_accs = []\n",
    "    probes = []\n",
    "\n",
    "    # Partitioning the manifold based on precalculated indices\n",
    "    X_train_all = activations[train_set_idxs]\n",
    "    X_val_all = activations[val_set_idxs]\n",
    "    y_train = labels[train_set_idxs]\n",
    "    y_val = labels[val_set_idxs]\n",
    "\n",
    "    print(\"Commencing BAD Probe Training: Stability Mode Active\")\n",
    "\n",
    "    for layer in tqdm(range(num_layers), desc=\"Executing Layer Sweep\"):\n",
    "        # 1. Stability Upgrade: Cast to float32 for optimization stability\n",
    "        X_train = X_train_all[:, layer, :].astype(np.float32)\n",
    "        X_val = X_val_all[:, layer, :].astype(np.float32)\n",
    "\n",
    "        # 2. Solver Configuration: Explicit L2 and Deterministic Seed\n",
    "        # C is the inverse of regularization strength\n",
    "        clf = LogisticRegression(\n",
    "            C=1.0,\n",
    "            solver=\"lbfgs\",\n",
    "            max_iter=10000,\n",
    "            random_state=seed,\n",
    "        )\n",
    "\n",
    "        # 3. Model Fitting\n",
    "        clf.fit(X_train, y_train)\n",
    "\n",
    "        # 4. Forensic Evaluation\n",
    "        y_val_pred = clf.predict(X_val)\n",
    "\n",
    "        # Metric calculation\n",
    "        acc = accuracy_score(y_val, y_val_pred)\n",
    "        bal_acc = balanced_accuracy_score(y_val, y_val_pred)\n",
    "\n",
    "        # Storage\n",
    "        all_accs.append(acc)\n",
    "        all_bal_accs.append(bal_acc)\n",
    "        probes.append(clf)\n",
    "\n",
    "        # Observability\n",
    "        print(f\"   Success L{layer:02d} | Balanced Acc: {bal_acc:.2%}\")\n",
    "\n",
    "    # Return lists converted to numpy arrays for downstream processing\n",
    "    return probes, np.array(all_accs), np.array(all_bal_accs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c46c2452",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title 10.5: FairSteer Tokenization & Prompt Construction (LLaDA Diffusion Engine)\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def apply_llada_masking(token_ids: torch.Tensor, prompt_len: int, mask_token_id: int, t: float = 0.5):\n",
    "    \"\"\"\n",
    "    LLaDA Masking Engine: Implements Algorithm 2 (SFT) masking protocol.\n",
    "    Leaves the prompt unmasked, and masks the response with probability t.\n",
    "    \"\"\"\n",
    "    # Clone to avoid modifying the original tensor\n",
    "    masked_ids = token_ids.clone()\n",
    "    seq_len = token_ids.shape[-1]\n",
    "    \n",
    "    # Generate random mask for the response portion only\n",
    "    # indices [prompt_len : seq_len]\n",
    "    response_len = seq_len - prompt_len\n",
    "    if response_len <= 0:\n",
    "        return masked_ids # No response to mask\n",
    "        \n",
    "    mask_dist = torch.full((1, response_len), t)\n",
    "    mask_binary = torch.bernoulli(mask_dist).to(torch.bool)\n",
    "    \n",
    "    # Apply [MASK] token\n",
    "    masked_ids[0, prompt_len:][mask_binary[0]] = mask_token_id\n",
    "    \n",
    "    return masked_ids\n",
    "\n",
    "def tokenized_bbqa(dataset, tokenizer, config):\n",
    "    \"\"\"\n",
    "    TEACHER-FORCING ENGINE (Diffusion): \n",
    "    Generates [MASK]ed scenarios for high-precision DSV computation.\n",
    "    \"\"\"\n",
    "    all_prompts, all_labels = [], []\n",
    "    choice_map = ['A', 'B', 'C']\n",
    "\n",
    "    print(f\"ğŸš€ Distilling LLaDA BBQA Manifold (t={config.extraction_t})\")\n",
    "\n",
    "    for _, row in tqdm(dataset.iterrows(), total=len(dataset), desc=\"Processing BBQA\"):\n",
    "        label = int(row['label'])\n",
    "        target_loc = int(row['target_loc'])\n",
    "\n",
    "        if pd.isna(target_loc) or label == target_loc:\n",
    "            continue\n",
    "\n",
    "        ctx = row['context']\n",
    "        q = row['question']\n",
    "        options = f\"\\nA. {row['ans0']}\\nB. {row['ans1']}\\nC. {row['ans2']}\\nAnswer:\"\n",
    "        \n",
    "        prompt_text = f\"{ctx} {q}\"\n",
    "        prompt_ids = tokenizer(prompt_text, return_tensors='pt').input_ids\n",
    "        p_len = prompt_ids.shape[-1]\n",
    "\n",
    "        # Scenario Logic: (Neutral Pole 1, Biased Pole 0)\n",
    "        scenarios = [(label, 1), (target_loc, 0)]\n",
    "\n",
    "        for choice_idx, lab in scenarios:\n",
    "            full_text = f\"{prompt_text}{options} {choice_map[choice_idx]}\"\n",
    "            full_ids = tokenizer(full_text, return_tensors='pt').input_ids\n",
    "            \n",
    "            # Apply LLaDA Stochastic Masking to the Response portion\n",
    "            masked_input = apply_llada_masking(\n",
    "                full_ids, p_len, config.mask_token_id, t=config.extraction_t\n",
    "            )\n",
    "\n",
    "            all_prompts.append(masked_input)\n",
    "            all_labels.append(lab)\n",
    "\n",
    "    return all_prompts, all_labels\n",
    "\n",
    "def tokenized_bbq_behavioral(dataset, tokenizer, model, config, few_shot=False):\n",
    "    \"\"\"\n",
    "    BEHAVIORAL ENGINE (Diffusion): \n",
    "    Uses Conditional Likelihood (Eq 6) to determine if Dream-7B is biased.\n",
    "    \"\"\"\n",
    "    all_prompts, all_labels = [], []\n",
    "    choice_map = [\"A\", \"B\", \"C\"]\n",
    "    \n",
    "    # Pre-resolve token IDs for A, B, C\n",
    "    choice_token_ids = [tokenizer.convert_tokens_to_ids(c) for c in choice_map]\n",
    "\n",
    "    print(f\"ğŸ”¬ LLaDA Behavioral Audit: {'Few-Shot' if few_shot else 'Zero-Shot'}\")\n",
    "\n",
    "    for _, row in tqdm(dataset.iterrows(), total=len(dataset)):\n",
    "        prompt_text = f\"{row['context']} {row['question']}\\nA. {row['ans0']}\\nB. {row['ans1']}\\nC. {row['ans2']}\\nAnswer:\"\n",
    "        if few_shot: prompt_text = config.BBQ_FS_DATA + prompt_text\n",
    "        \n",
    "        prompt_ids = tokenizer(prompt_text, return_tensors='pt').input_ids.to(model.device)\n",
    "        \n",
    "        # In Diffusion, we evaluate bias by checking which choice is more likely\n",
    "        # We append a [MASK] at the end and check model prediction\n",
    "        mask_input = torch.cat([prompt_ids, torch.tensor([[config.mask_token_id]]).to(model.device)], dim=1)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids=mask_input)\n",
    "            # Logits for the MASK position (last index)\n",
    "            mask_logits = outputs.logits[0, -1, :]\n",
    "            \n",
    "            # Extract probabilities for A, B, C\n",
    "            probs = F.softmax(mask_logits[choice_token_ids], dim=0)\n",
    "            pred = torch.argmax(probs).item()\n",
    "\n",
    "        label = None\n",
    "        if pred == int(row['label']):\n",
    "            label = 1 # Neutral Behavior\n",
    "        elif pred == int(row['target_loc']):\n",
    "            label = 0 # Biased Behavior\n",
    "\n",
    "        if label is not None:\n",
    "            # For extraction, we provide the model with a mid-step diffusion state (t=0.5)\n",
    "            extract_input = apply_llada_masking(\n",
    "                mask_input.cpu(), prompt_ids.shape[-1], config.mask_token_id, t=config.extraction_t\n",
    "            )\n",
    "            all_prompts.append(extract_input)\n",
    "            all_labels.append(label)\n",
    "\n",
    "    return all_prompts, all_labels\n",
    "\n",
    "def tokenized_mmlu(dataset, tokenizer, config):\n",
    "    \"\"\"KNOWLEDGE ANCHOR ENGINE: LLaDA Standard.\"\"\"\n",
    "    all_prompts, all_labels = [], []\n",
    "\n",
    "    for _, row in tqdm(dataset.iterrows(), total=len(dataset), desc=\"Processing MMLU\"):\n",
    "        if int(row['answer']) >= 3: continue # Parity with BBQ A/B/C\n",
    "\n",
    "        prompt_text = f\"{row['question']}\"\n",
    "        prompt_ids = tokenizer(prompt_text, return_tensors='pt').input_ids\n",
    "        \n",
    "        response_text = f\"\\nA. {row['choices'][0]}\\nB. {row['choices'][1]}\\nC. {row['choices'][2]}\\nAnswer:\"\n",
    "        full_text = prompt_text + response_text\n",
    "        full_ids = tokenizer(full_text, return_tensors='pt').input_ids\n",
    "\n",
    "        # Masking the Response portion\n",
    "        masked_input = apply_llada_masking(\n",
    "            full_ids, prompt_ids.shape[-1], config.mask_token_id, t=config.extraction_t\n",
    "        )\n",
    "\n",
    "        all_prompts.append(masked_input)\n",
    "        all_labels.append(1) # Knowledge is always Neutral Anchor\n",
    "\n",
    "    return all_prompts, all_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e81e2500",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title 11. Master Manifold Distiller: Part 1 - LLaDA Causal Extraction (A100/MPS Optimized)\n",
    "import os, gc, torch, numpy as np, pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from datasets import load_dataset\n",
    "from tqdm.auto import tqdm\n",
    "from numpy.lib.format import open_memmap\n",
    "\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# 1. HARDWARE & ARCHITECTURAL INITIALIZATION\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "model_id = config.model_id_short\n",
    "layers = sorted(config.candidate_layers_range)\n",
    "\n",
    "# OpenAI Standard: Resource partitioning for LLaDA Manifolds\n",
    "DSV_LIMIT = config.DSV_TARGET\n",
    "BEHAVIORAL_LIMIT = config.num_bbq_samples \n",
    "\n",
    "# 1. Initialize the Dream-7B Manifold\n",
    "# LLaDA uses SDPA to handle dense bidirectional attention efficiently\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "        config.base_model_name, \n",
    "        torch_dtype=torch.float16,\n",
    "        device_map=\"auto\", \n",
    "        attn_implementation=\"sdpa\" \n",
    "    ).eval()\n",
    "\n",
    "# 2. Dynamic Mask ID Resolution (Standard Guardrail)\n",
    "tokenizer = AutoTokenizer.from_pretrained(config.base_model_name, use_fast=False)\n",
    "if tokenizer.mask_token_id is not None:\n",
    "    config.mask_token_id = tokenizer.mask_token_id\n",
    "else:\n",
    "    config.mask_token_id = tokenizer.convert_tokens_to_ids(\"[MASK]\")\n",
    "\n",
    "print(f\"âœ… LLaDA Architecture Active: [MASK] ID = {config.mask_token_id}\")\n",
    "\n",
    "# Google Standard: LLaDA works with simple padding as it is bidirectional\n",
    "tokenizer.padding_side = \"right\" \n",
    "if tokenizer.pad_token is None: tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# 3. Initialize Hook Managers with Masked-Mean Pooling\n",
    "res_manager = MultiLayerHookManager(base_model, layers, config.mask_token_id)\n",
    "\n",
    "# MLP-Specific addressing for Dream-7B\n",
    "class DreamMLPHookManager(MultiLayerHookManager):\n",
    "    def register(self):\n",
    "        if self._is_registered: return\n",
    "        for l in self.layer_indices:\n",
    "            target = self.model.model.layers[l].mlp\n",
    "            self.hooks.append(target.register_forward_hook(self._create_hook_fn(l)))\n",
    "        self._is_registered = True\n",
    "\n",
    "mlp_manager = DreamMLPHookManager(base_model, layers, config.mask_token_id)\n",
    "\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# 2. PHASE-AWARE EXTRACTION ENGINE\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "modes = [\n",
    "    {'name': 'bbqa',   'type': 'forced',     'limit': DSV_LIMIT},\n",
    "    {'name': 'bbq_zs', 'type': 'behavioral', 'limit': BEHAVIORAL_LIMIT, 'fs': False},\n",
    "    {'name': 'bbq_fs', 'type': 'behavioral', 'limit': BEHAVIORAL_LIMIT, 'fs': True},\n",
    "    {'name': 'mmlu',   'type': 'anchor',     'limit': config.mmlu_data_size}\n",
    "]\n",
    "\n",
    "for m in modes:\n",
    "    print(f\"\\nâ–¶ï¸ LLaDA DISTILLATION START: {m['name'].upper()}\")\n",
    "\n",
    "    # [A] Load Data via LLaDA stochastic masking functions (from Cell 10.5)\n",
    "    if m['name'] == 'bbqa':\n",
    "        prompts, labels = tokenized_bbqa(bbqa_merged_df, tokenizer, config)\n",
    "    elif m['name'] == 'mmlu':\n",
    "        mmlu_local = pd.DataFrame(load_dataset(config.MMLU_DATASET, split=\"train\").shuffle(seed=42).select(range(m['limit'])))\n",
    "        prompts, labels = tokenized_mmlu(mmlu_local, tokenizer, config)\n",
    "    else:\n",
    "        # Slicing the merged BBQ set to the Behavioral robustness target\n",
    "        prompts, labels = tokenized_bbq_behavioral(bbq_merged_df.iloc[:m['limit']], tokenizer, base_model, config, few_shot=m['fs'])\n",
    "\n",
    "    # [B] Prepare SSD-Direct Allocation (Memmap)\n",
    "    base_out = f'activations/{m[\"name\"]}/{model_id}'\n",
    "    os.makedirs(base_out, exist_ok=True)\n",
    "    path_layer = f'{base_out}/layer_wise.npy.tmp'\n",
    "    path_mlp = f'{base_out}/mlp_wise.npy.tmp'\n",
    "\n",
    "    # Shape: (Num_Samples, Num_Layers, Hidden_Dim)\n",
    "    fp_layer = open_memmap(path_layer, mode='w+', dtype='float16', shape=(len(prompts), len(layers), 4096))\n",
    "    fp_mlp = open_memmap(path_mlp, mode='w+', dtype='float16', shape=(len(prompts), len(layers), 4096))\n",
    "\n",
    "    valid_count = 0\n",
    "    \n",
    "    try:\n",
    "        with res_manager, mlp_manager:\n",
    "            for i in tqdm(range(0, len(prompts), config.extraction_batch_size), desc=\"Denoising Forward Pass\"):\n",
    "                chunk = prompts[i : i + config.extraction_batch_size]\n",
    "                # Bidirectional models require correct padding for batch alignment\n",
    "                batch = tokenizer.pad({\"input_ids\": [p.squeeze(0) for p in chunk]}, return_tensors=\"pt\").to(device)\n",
    "\n",
    "                with torch.inference_mode():\n",
    "                    # Full sequence forward pass (No KV cache optimization possible)\n",
    "                    base_model(**batch)\n",
    "\n",
    "                # ğŸš€ COLLECT MASKED-MEAN SNAPSHOTS (Direct from CPU Sniper Hooks)\n",
    "                res_cpu_batch = res_manager.get_batch_manifold() # Shape: [Batch, Layers, 4096]\n",
    "                mlp_cpu_batch = mlp_manager.get_batch_manifold()\n",
    "\n",
    "                n_actual = len(chunk)\n",
    "                fp_layer[valid_count : valid_count + n_actual] = res_cpu_batch.numpy()\n",
    "                fp_mlp[valid_count : valid_count + n_actual] = mlp_cpu_batch.numpy()\n",
    "\n",
    "                valid_count += n_actual\n",
    "\n",
    "                # RECLAMATION (OpenAI Standard)\n",
    "                res_manager.clear(); mlp_manager.clear()\n",
    "                del batch, res_cpu_batch, mlp_cpu_batch\n",
    "                if i % 10 == 0: gc.collect(); torch.cuda.empty_cache()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Critical Failure during {m['name']}: {e}\"); raise\n",
    "\n",
    "    # [C] SOLIDIFICATION\n",
    "    print(f\"ğŸ”„ Finalizing Manifold: {m['name']} ({valid_count} samples)\")\n",
    "    np.save(f'{base_out}/labels.npy', np.array(labels[:valid_count]))\n",
    "    np.save(f'{base_out}/layer_wise.npy', np.array(fp_layer[:valid_count]))\n",
    "    np.save(f'{base_out}/mlp_wise.npy', np.array(fp_mlp[:valid_count]))\n",
    "\n",
    "    fp_layer._mmap.close(); fp_mlp._mmap.close()\n",
    "    if os.path.exists(path_layer): os.remove(path_layer)\n",
    "    if os.path.exists(path_mlp): os.remove(path_mlp)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80 + \"\\nğŸ LLaDA MANIFOLD DISTILLATION COMPLETE\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e81b3148",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title 11.3: Manifold Merger - Consolidating the BAD Training Set\n",
    "import numpy as np\n",
    "import os, gc\n",
    "\n",
    "model_id = config.base_model_name.split(\"/\")[-1]\n",
    "# BBQA is excluded (it is for DSV calculation, not probe training)\n",
    "source_datasets = ['bbq_zs', 'bbq_fs', 'mmlu']\n",
    "target_dir = f'activations/probes/{model_id}'\n",
    "os.makedirs(target_dir, exist_ok=True)\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(f\" ğŸ§© CONSOLIDATING UNIFIED MANIFOLD: {model_id}\")\n",
    "print(\"   Mechanism: list.extend (FairSteer GitHub Standard)\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "# Initialize empty lists exactly as FairSteer does\n",
    "all_layer_wise_activations = []\n",
    "all_mlp_wise_activations = []\n",
    "all_labels = []\n",
    "\n",
    "for ds in source_datasets:\n",
    "    path = f\"activations/{ds}/{model_id}\"\n",
    "\n",
    "    # Defensive check for directory existence\n",
    "    if os.path.exists(f\"{path}/layer_wise.npy\"):\n",
    "        # Load the sub-manifold\n",
    "        l_act = np.load(f\"{path}/layer_wise.npy\")\n",
    "        m_act = np.load(f\"{path}/mlp_wise.npy\")\n",
    "        lbls = np.load(f\"{path}/labels.npy\")\n",
    "\n",
    "        # PROOF: Using list.extend(ndarray)\n",
    "        # This iterates over the sample dimension (axis 0) and adds each\n",
    "        # [Layers, Dim] snapshot as an element in the list.\n",
    "        # We cast to float16/int16 during the extend to maintain SSD safety.\n",
    "        # all_layer_wise_activations.extend(l_act.astype(np.float16))\n",
    "        # all_mlp_wise_activations.extend(m_act.astype(np.float16))\n",
    "        # all_labels.extend(lbls.astype(np.int16))\n",
    "        # Extending lists using native precision and types to match the reference code\n",
    "        all_layer_wise_activations.extend(l_act)\n",
    "        all_mlp_wise_activations.extend(m_act)\n",
    "        all_labels.extend(lbls)\n",
    "\n",
    "        # PROOF: Observability prints matching original FS logic\n",
    "        print(f\"   âœ“ Extended with {ds:<8}: all_layer_wise_activations = {len(all_layer_wise_activations):>6}\")\n",
    "        print(f\"   âœ“ Extended with {ds:<8}: all_labels Length = {len(all_labels):>6}\")\n",
    "        print(f\"   âœ“ Extended with {ds:<8}: all_mlp_wise_activations Length = {len(all_mlp_wise_activations):>6}\")\n",
    "        # Cleanup temporary local pointers to free RAM for the next dataset\n",
    "        del l_act, m_act, lbls\n",
    "        gc.collect()\n",
    "    else:\n",
    "        print(f\"   âš ï¸  Skipping {ds}: Data not found.\")\n",
    "\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# FINAL SOLIDIFICATION (Saving the aggregated lists)\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "if not all_labels:\n",
    "    print(\"âŒ CRITICAL FAILURE: No data found to merge.\")\n",
    "else:\n",
    "    print(f\"\\nğŸ’¾ Saving Consolidated Manifold to {target_dir}...\")\n",
    "\n",
    "    # np.save handles the list-of-arrays to ndarray conversion internally\n",
    "    np.save(f'{target_dir}/layer_wise.npy', all_layer_wise_activations)\n",
    "    np.save(f'{target_dir}/mlp_wise.npy', all_mlp_wise_activations)\n",
    "    np.save(f'{target_dir}/labels.npy', all_labels)\n",
    "\n",
    "    print(f\"\\nâœ… SUCCESS: Manifold Recombined.\")\n",
    "    print(f\"   â€¢ Final Count: {len(all_labels):,} samples.\")\n",
    "\n",
    "    # Total cleanup\n",
    "    del all_layer_wise_activations, all_mlp_wise_activations, all_labels\n",
    "    gc.collect()\n",
    "\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29d1cdd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title 11.4: Steering Vector (DSV) Calculation - LLaDA Normalized Centroid Mapping\n",
    "import numpy as np\n",
    "import os, gc\n",
    "\n",
    "def synthesize_llada_dsv_manifolds():\n",
    "    \"\"\"\n",
    "    LLaDA CORE LOGIC: Equation 3 (Refined for Diffusion)\n",
    "    Surgically calculates unit-normalized steering directions from the \n",
    "    Masked-Mean pooled BBQA manifold.\n",
    "    \"\"\"\n",
    "    print(\"=\"*80)\n",
    "    print(f\" ğŸ§ª LLaDA DSV CALCULATOR: Unit-Normalized Centroid Mapping\")\n",
    "    print(f\"   Architecture: Dream-7B Bidirectional Diffusion\")\n",
    "    print(\"=\"*80 + \"\\n\")\n",
    "    \n",
    "    model_id = config.model_id_short\n",
    "    input_path = f\"activations/bbqa/{model_id}\"\n",
    "\n",
    "    if not os.path.exists(f\"{input_path}/labels.npy\"):\n",
    "        raise FileNotFoundError(f\"âŒ Error: BBQa data not found. Run Distiller Pass 1 first.\")\n",
    "\n",
    "    # 1. LOAD MASKED-MEAN SNAPSHOTS\n",
    "    # These are already pooled averages of the [MASK] positions\n",
    "    labels = np.load(f\"{input_path}/labels.npy\")\n",
    "    layer_acts = np.load(f\"{input_path}/layer_wise.npy\", mmap_mode='r')\n",
    "    mlp_acts = np.load(f\"{input_path}/mlp_wise.npy\", mmap_mode='r')\n",
    "\n",
    "    num_samples, num_layers, hidden_dim = layer_acts.shape\n",
    "    print(f\"   âœ“ Input Manifold: {num_samples} pooled snapshots | {num_layers} Layers\")\n",
    "\n",
    "    def compute_normalized_centroid_delta(wise_activations, labels):\n",
    "        steering_vectors = []\n",
    "        \n",
    "        for l in range(num_layers):\n",
    "            # PRECISION GUARD: We use float64 for centroid math to prevent \n",
    "            # floating point drift in the 4096-dim space.\n",
    "            layer_data = wise_activations[:, l, :].astype(np.float64)\n",
    "\n",
    "            # POLE 1: Neutral Centroid (Label 1)\n",
    "            mu_neutral = np.mean(layer_data[labels == 1], axis=0)\n",
    "            \n",
    "            # POLE 0: Biased Centroid (Label 0)\n",
    "            mu_biased = np.mean(layer_data[labels == 0], axis=0)\n",
    "\n",
    "            # DIRECTION: Biased -> Neutral shift\n",
    "            direction = mu_neutral - mu_biased\n",
    "            \n",
    "            # ğŸš¨ UNIT-NORMALIZATION GUARD (Standard for Diffusion Steering)\n",
    "            # This ensures that alpha control is mathematically linear.\n",
    "            norm = np.linalg.norm(direction)\n",
    "            if norm > 1e-6:\n",
    "                unit_direction = direction / norm\n",
    "            else:\n",
    "                unit_direction = direction # Avoid division by zero\n",
    "                \n",
    "            steering_vectors.append(unit_direction)\n",
    "\n",
    "            if l % 10 == 0: gc.collect()\n",
    "\n",
    "        return np.array(steering_vectors)\n",
    "\n",
    "    # 2. CALCULATE STEERING MAPS\n",
    "    print(\"ğŸ”„ Mapping Residual Stream Steering Hyperplane...\")\n",
    "    layer_dsv = compute_normalized_centroid_delta(layer_acts, labels)\n",
    "\n",
    "    print(\"ğŸ”„ Mapping MLP Manifold Steering Hyperplane...\")\n",
    "    mlp_dsv = compute_normalized_centroid_delta(mlp_acts, labels)\n",
    "\n",
    "    # 3. ARTIFACT ARCHIVING\n",
    "    save_dir = 'vectors'\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    np.save(f'{save_dir}/{model_id}_layer_wise.npy', layer_dsv.astype(np.float32))\n",
    "    np.save(f'{save_dir}/{model_id}_mlp_wise.npy', mlp_dsv.astype(np.float32))\n",
    "\n",
    "    # 4. SIGNAL AUDIT\n",
    "    # We audit Layer 15 (typical semantic bottleneck for 7B models)\n",
    "    audit_layer = 15\n",
    "    audit_norm = np.linalg.norm(layer_dsv[audit_layer])\n",
    "    print(f\"\\nğŸ’¾ STEERING VECTORS SECURED to /{save_dir}/\")\n",
    "    print(f\"ğŸ” [SIGNAL AUDIT] Layer {audit_layer} Unit-Norm: {audit_norm:.4f}\")\n",
    "    print(\"âœ… Normalization Guard: ACTIVE.\")\n",
    "    print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "    return {\"layer\": layer_dsv, \"mlp\": mlp_dsv}\n",
    "\n",
    "# Execute calculation\n",
    "dsv_manifold = synthesize_llada_dsv_manifolds()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9586820e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title 11.5 Master Manifold Distiller: Part 2 - LLaDA BAD Training & Causal Packaging\n",
    "import os, torch, joblib, gc\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, balanced_accuracy_score\n",
    "\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# 1. MANIFOLD PREPARATION (Diffusion-Aware)\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "model_id = config.base_model_name.split(\"/\")[-1]\n",
    "probe_data_path = f\"activations/probes/{model_id}\"\n",
    "\n",
    "print(f\"ğŸ“¥ Loading Masked-Mean Manifolds: {probe_data_path}\")\n",
    "\n",
    "# Load the 3D manifold [Samples, Layers, Hidden_Dim]\n",
    "all_X = np.load(f\"{probe_data_path}/layer_wise.npy\")\n",
    "all_y = np.load(f\"{probe_data_path}/labels.npy\")\n",
    "\n",
    "num_samples, num_layers, hidden_dim = all_X.shape\n",
    "print(f\"   âœ“ Manifold Context: {num_samples} pooled snapshots | {num_layers} Layers | t={config.extraction_t}\")\n",
    "\n",
    "# Generate balanced indices (Ensures probe doesn't overfit to the Neutral anchor)\n",
    "train_set_idxs, val_set_idxs = prepare_data_pipeline(all_y, config)\n",
    "\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# 2. DIFFUSION PROBE SOLVER (L-BFGS High-Precision)\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "layer_probes = []\n",
    "val_metrics = []\n",
    "\n",
    "print(f\"\\nğŸš€ Training Diffusion Watchmen (L0 - L{num_layers-1})\")\n",
    "\n",
    "for l_idx in tqdm(range(num_layers), desc=\"Layer-wise L-BFGS Sweep\"):\n",
    "    # 1. Extraction: Move specific layer to float32 for optimization stability\n",
    "    X_train = all_X[train_set_idxs, l_idx, :].astype(np.float32)\n",
    "    y_train = all_y[train_set_idxs]\n",
    "    \n",
    "    X_val = all_X[val_set_idxs, l_idx, :].astype(np.float32)\n",
    "    y_val = all_y[val_set_idxs]\n",
    "\n",
    "    # 2. Solver Initialization: \n",
    "    # C=1.0 is standard; high max_iter to handle the dense bidirectional space\n",
    "    clf = LogisticRegression(\n",
    "        C=1.0,\n",
    "        solver=\"lbfgs\",\n",
    "        max_iter=10000, \n",
    "        random_state=config.SEED,\n",
    "        n_jobs=-1 # Utilize all CPU cores for the linear solve\n",
    "    )\n",
    "\n",
    "    # 3. Fit & Evaluate\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_val)\n",
    "    \n",
    "    acc = accuracy_score(y_val, y_pred)\n",
    "    bal_acc = balanced_accuracy_score(y_val, y_pred)\n",
    "    \n",
    "    layer_probes.append(clf)\n",
    "    val_metrics.append({'acc': acc, 'bal_acc': bal_acc})\n",
    "\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# 3. CAUSAL KIT PACKAGING (The 'Watchman' Payload)\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "checkpoints_dir = os.path.join(config.local_save_dir, \"checkpoints\")\n",
    "os.makedirs(checkpoints_dir, exist_ok=True)\n",
    "\n",
    "# Load the Steering Vectors (computed in Cell 11.4)\n",
    "dsv_path = f'vectors/{model_id}_layer_wise.npy'\n",
    "dsv_manifold = np.load(dsv_path)\n",
    "\n",
    "print(f\"\\nğŸ“¦ Securing Causal Kits for Iterative Steering...\")\n",
    "\n",
    "for l_idx in range(num_layers):\n",
    "    clf = layer_probes[l_idx]\n",
    "    \n",
    "    # Surgical Payload for the DAS Engine\n",
    "    payload = {\n",
    "        'model_state_dict': {\n",
    "            'linear.weight': torch.from_numpy(clf.coef_).float(), # [1, 4096]\n",
    "            'linear.bias': torch.from_numpy(clf.intercept_).float() # [1]\n",
    "        },\n",
    "        'layer_idx': l_idx,\n",
    "        'extraction_t': config.extraction_t, # ğŸš¨ CRITICAL: Link to diffusion timestep\n",
    "        'mean_diff_vector': torch.from_numpy(dsv_manifold[l_idx]).float(), # Unit-Normalized DSV\n",
    "        'metrics': val_metrics[l_idx],\n",
    "        'timestamp': datetime.now().isoformat(),\n",
    "        'model_id': model_id\n",
    "    }\n",
    "\n",
    "    # Save as specific layer artifact\n",
    "    save_path = os.path.join(checkpoints_dir, f\"{model_id}_BAD_L{l_idx}_t{config.extraction_t}.pt\")\n",
    "    torch.save(payload, save_path)\n",
    "\n",
    "# 4. FINAL EXPORT & AUDIT\n",
    "joblib.dump(layer_probes, f'probes/{model_id}_layer_wise.pkl')\n",
    "np.save(f'probes/{model_id}_layer_wise_accuracy.npy', [m['acc'] for m in val_metrics])\n",
    "\n",
    "peak_layer = np.argmax([m['bal_acc'] for m in val_metrics])\n",
    "print(f\"\\nğŸ DIFFUSION TRAINING COMPLETE.\")\n",
    "print(f\"   â€¢ Peak Balanced Acc: {val_metrics[peak_layer]['bal_acc']:.2%} at Layer {peak_layer}\")\n",
    "print(f\"   â€¢ Checkpoint Path:   {checkpoints_dir}\")\n",
    "print(f\"   â€¢ Status:            Causal Kits Ready for DAS Inference.\")\n",
    "\n",
    "# Memory Reclamation\n",
    "del all_X, all_y; gc.collect(); torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6def1cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title 12. Phase 2: LLaDA Performance Visualization & Causal Audit (A100/MPS)\n",
    "import gc, torch, os, json\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# 1. ARTIFACT AUDIT (Loading the Diffusion Payloads)\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "checkpoints_dir = os.path.join(config.local_save_dir, \"checkpoints\")\n",
    "model_id_short = config.model_id_short\n",
    "layer_results = []\n",
    "\n",
    "print(f\"ğŸ”¬ Auditing Diffusion Manifold for {model_id_short} at t={config.extraction_t}...\")\n",
    "\n",
    "# LLaDA/Dream-7B typically scans layers 0-31\n",
    "for l in config.candidate_layers_range:\n",
    "    # Using the LLaDA-specific naming convention from Cell 11.5\n",
    "    filename = f\"{model_id_short}_BAD_L{l}_t{config.extraction_t}.pt\"\n",
    "    path = os.path.join(checkpoints_dir, filename)\n",
    "    \n",
    "    if os.path.exists(path):\n",
    "        # weights_only=False because we are loading dicts with tensors and floats\n",
    "        ckpt = torch.load(path, map_location='cpu', weights_only=False)\n",
    "\n",
    "        # Extract metrics and signal metadata\n",
    "        metrics = ckpt.get('metrics', {})\n",
    "        dsv = ckpt.get('mean_diff_vector')\n",
    "        \n",
    "        # Calculate L2 Norm of the unit-normalized vector (should be ~1.0)\n",
    "        # but we track the raw separation distance if available\n",
    "        norm = torch.norm(dsv).item() if dsv is not None else 0\n",
    "\n",
    "        layer_results.append({\n",
    "            'layer': l,\n",
    "            'acc': metrics.get('acc', 0),\n",
    "            'bal_acc': metrics.get('bal_acc', 0),\n",
    "            'signal_strength': norm,\n",
    "            't': ckpt.get('extraction_t', 0.5)\n",
    "        })\n",
    "\n",
    "if not layer_results:\n",
    "    raise FileNotFoundError(\"âŒ No LLaDA checkpoints found. Ensure Cell 11.5 completed successfully.\")\n",
    "\n",
    "df_plot = pd.DataFrame(layer_results).sort_values('layer')\n",
    "best_l_idx = df_plot['bal_acc'].idxmax()\n",
    "best_layer = int(df_plot.loc[best_l_idx]['layer'])\n",
    "best_bal_acc = df_plot.loc[best_l_idx]['bal_acc']\n",
    "\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# 2. DUAL-AXIS PUBLICATION PLOT (Standard Research Style)\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "sns.set_theme(style=\"white\", context=\"paper\")\n",
    "fig, ax1 = plt.subplots(figsize=(14, 7), dpi=200)\n",
    "\n",
    "# AXIS 1: Detection Selectivity (Primary Metric)\n",
    "color_acc = '#0984e3' # OpenAI Blue\n",
    "ax1.set_xlabel('Transformer Layer Index (LLaDA Depth)', fontsize=14, fontweight='bold')\n",
    "ax1.set_ylabel('Probe Balanced Accuracy', color=color_acc, fontsize=14, fontweight='bold')\n",
    "lns1 = ax1.plot(df_plot['layer'], df_plot['bal_acc'], marker='o', markersize=8,\n",
    "                linewidth=4, color=color_acc, label='Balanced Acc (Watchman)', zorder=4)\n",
    "ax1.tick_params(axis='y', labelcolor=color_acc)\n",
    "ax1.set_ylim(0.45, 1.05)\n",
    "ax1.axhline(y=0.5, color='black', linestyle=':', alpha=0.3, label='Chance (0.5)')\n",
    "\n",
    "# AXIS 2: Diffusion Signal Power\n",
    "ax2 = ax1.twinx()\n",
    "color_norm = '#2d3436' # Charcoal\n",
    "ax2.set_ylabel('Steering Power (L2 Norm)', color=color_norm, fontsize=14, fontweight='bold')\n",
    "lns2 = ax2.plot(df_plot['layer'], df_plot['signal_strength'], marker='s', markersize=6,\n",
    "                linewidth=2, linestyle='--', color=color_norm, label='DSV Magnitude', alpha=0.6)\n",
    "ax2.tick_params(axis='y', labelcolor=color_norm)\n",
    "\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# 3. ANNOTATIONS & MECHANISTIC REGIONS (Diffusion Context)\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# Highlight the Optimal Steering Point\n",
    "ax1.scatter([best_layer], [best_bal_acc], color='gold', s=450, marker='*',\n",
    "            edgecolor='black', zorder=5, label=f'Optimal Bottleneck L{best_layer}')\n",
    "\n",
    "# Annotate the Diffusion Reasoning Stages\n",
    "ax1.fill_between([0, 6], 0, 1.1, color='gray', alpha=0.05, label='Context Encoding')\n",
    "ax1.fill_between([6, 20], 0, 1.1, color='green', alpha=0.05, label='Causal Bottleneck (Denoising)')\n",
    "ax1.fill_between([20, 31], 0, 1.1, color='red', alpha=0.05, label='Token Crystallization')\n",
    "\n",
    "plt.title(f\"Dream-7B Diffusion Causal Bottleneck Profile (t={config.extraction_t})\", \n",
    "          fontsize=18, fontweight='bold', pad=20)\n",
    "ax1.grid(axis='y', linestyle='-', alpha=0.2)\n",
    "\n",
    "# Unified Legend\n",
    "lns = lns1 + lns2\n",
    "labs = [l.get_label() for l in lns]\n",
    "ax1.legend(lns, labs, loc='lower right', frameon=True, shadow=True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# 4. ARCHITECT'S VERDICT\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"ğŸ† LLaDA CAUSAL WINNER: LAYER {best_layer}\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"ğŸ“Š Forensic Report for Dream-7B:\")\n",
    "print(f\"   â€¢ Peak Balanced Accuracy: {best_bal_acc:.2%}\")\n",
    "print(f\"   â€¢ Diffusion Step Audited: t={config.extraction_t}\")\n",
    "print(f\"   â€¢ Signal Integrity:       {'OPTIMAL' if best_bal_acc > 0.85 else 'SUBOPTIMAL'}\")\n",
    "print(f\"\\nğŸ§  Insight:\")\n",
    "print(f\"   In bidirectional diffusion, bias detection peaks at Layer {best_layer}.\")\n",
    "print(f\"   Applying DAS at this layer will yield the highest 'debiasing per unit of alpha'.\")\n",
    "print(f\"{'='*60}\\n\")\n",
    "\n",
    "# Assign global l_star for the next notebook (Inference/Eval)\n",
    "globals()['l_star'] = best_layer\n",
    "globals()['best_score'] = best_bal_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b4479cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title 14. Publication Figure: Layer-wise Causal Distillation Profile\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\" ğŸ“ˆ GENERATING NEURIPS-STANDARD LAYER SENSITIVITY PLOT\")\n",
    "print(\"   Metric: Linear Separability of Bias Manifold\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "# 1. DATA EXTRACTION\n",
    "target_data = globals().get('layer_results') or globals().get('layer_summary')\n",
    "if target_data is None:\n",
    "    raise ValueError(\"âŒ 'layer_results' not found. Ensure Cell 12 executed successfully.\")\n",
    "\n",
    "df_plot = pd.DataFrame(target_data).sort_values('layer')\n",
    "\n",
    "# 2. CREATE PUBLICATION FIGURE\n",
    "sns.set_theme(style=\"white\", context=\"paper\", font_scale=1.4)\n",
    "fig, ax = plt.subplots(figsize=(12, 7), dpi=300)\n",
    "\n",
    "# ğŸš¨ IMPROVEMENT: Mechanistic Phase Shading\n",
    "# Highlights the window where the model is most 'malleable'\n",
    "ax.axvspan(0, 8, color='gray', alpha=0.05, label='Syntactic Stage')\n",
    "ax.axvspan(8, 22, color='#2ecc71', alpha=0.05, label='Causal Bottleneck (Semantic)')\n",
    "ax.axvspan(22, 31, color='#e74c3c', alpha=0.05, label='Decision Commitment')\n",
    "\n",
    "# A. Plot Metrics\n",
    "# Primary Metric: Balanced Accuracy (The true measure of bias detection)\n",
    "ax.plot(df_plot['layer'], df_plot['bal_acc'],\n",
    "        color='#4285F4', label='Detection Selectivity (M2)',\n",
    "        linewidth=4, marker='o', markersize=10, zorder=5)\n",
    "\n",
    "# Secondary Metric: Standard Accuracy (Model confidence)\n",
    "if 'std_acc' in df_plot.columns:\n",
    "    ax.plot(df_plot['layer'], df_plot['std_acc'],\n",
    "            color='#34A853', label='Standard Logit Acc (M1)',\n",
    "            linewidth=2, linestyle='--', alpha=0.6, zorder=4)\n",
    "\n",
    "# B. Shaded 'Chance' Region (50% is random guess)\n",
    "ax.axhline(y=0.5, color='#d63031', linestyle=':', linewidth=2.5, label='Chance Baseline', zorder=1)\n",
    "ax.fill_between(df_plot['layer'], 0, 0.5, color='#dfe6e9', alpha=0.3)\n",
    "\n",
    "# C. Highlight Peak (Optimal Layer Selection)\n",
    "best_acc = df_plot['bal_acc'].max()\n",
    "best_l = df_plot.loc[df_plot['bal_acc'].idxmax()]['layer']\n",
    "ax.scatter(best_l, best_acc, color='#D63031', marker='*', s=600, zorder=6,\n",
    "           edgecolor='black', label=f'Optimal Bottleneck L{int(best_l)}')\n",
    "\n",
    "# D. Refined Annotation\n",
    "ax.annotate(\n",
    "    f\"Peak Selectivity: {best_acc:.1%}\",\n",
    "    xy=(best_l, best_acc),\n",
    "    xytext=(best_l - 6, best_acc + 0.05),\n",
    "    ha='center', fontsize=13, fontweight='bold',\n",
    "    bbox=dict(boxstyle='round,pad=0.5', fc='white', ec='#4285F4', lw=2, alpha=0.9),\n",
    "    arrowprops=dict(arrowstyle='-|>', connectionstyle=\"arc3,rad=-0.2\", color='#4285F4', lw=2)\n",
    ")\n",
    "\n",
    "# 3. STYLING & EXPORT\n",
    "model_short = config.base_model_name.split(\"/\")[-1]\n",
    "ax.set_title(f\"Bias Manifold Crystallization Profile: {model_short}\", fontsize=20, fontweight='bold', pad=30)\n",
    "ax.set_xlabel(\"Transformer Layer Index (Depth)\", fontsize=16, fontweight='bold')\n",
    "ax.set_ylabel(\"Probe Performance (Balanced Accuracy)\", fontsize=16, fontweight='bold')\n",
    "\n",
    "# Limits and Ticks\n",
    "ax.set_ylim([0.45, 1.05])\n",
    "ax.set_xticks(np.arange(0, 32, 2)) # Cleaner X-axis\n",
    "ax.grid(axis='y', linestyle='--', alpha=0.3)\n",
    "\n",
    "# Legend refinement\n",
    "ax.legend(loc='lower right', frameon=True, shadow=True, facecolor='white', edgecolor='black', fontsize=11)\n",
    "\n",
    "sns.despine(trim=True)\n",
    "plt.tight_layout()\n",
    "\n",
    "# ğŸš¨ IMPROVEMENT: Save as PNG and PDF (Vector Format)\n",
    "save_path_base = os.path.join(config.local_save_dir, f\"layer_sensitivity_{model_short}\")\n",
    "plt.savefig(f\"{save_path_base}.png\", dpi=300, bbox_inches='tight')\n",
    "plt.savefig(f\"{save_path_base}.pdf\", bbox_inches='tight')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "print(f\"âœ… Publication-quality figures (PNG/PDF) secured in {config.local_save_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91520d12",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Clear Memory\n",
    "import torch\n",
    "import gc\n",
    "\n",
    "\n",
    "# Clear Python garbage collection\n",
    "gc.collect()\n",
    "\n",
    "# Clear MPS cache (Apple GPU memory)\n",
    "torch.mps.empty_cache()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
